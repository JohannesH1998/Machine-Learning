{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87dd0cd",
   "metadata": {},
   "source": [
    "# Studienarbeit: Machine Learning\n",
    "#### Saniye Ogul und Johannes Horst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6569b20e",
   "metadata": {},
   "source": [
    "### Imports\n",
    "- plotly\n",
    "- matplotlib\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c4f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "#%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#import graphviz\n",
    "from sklearn import tree\n",
    "from matplotlib.dates import DateFormatter\n",
    "import datetime as dt\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71b9d202",
   "metadata": {},
   "source": [
    "### Aufgabe 1 (Klassifikation des Systemzustands)\n",
    "Erstellung eines binären Klassifikationsmodell zur Vorhersage des aktuellen Zutands der APU (Air Production Unit) auf Basis der Sensordaten. Soll differenziert werden ob APU in Ordnung oder nicht ist (binäre Klassifikation)\n",
    "zu verwendene Verfahren:\n",
    "- einen nicht-parametrisierten Modellansatz,\n",
    "- einen parametrisierten Modellansatz,\n",
    "- ein Verfahren aus dem Bereich des Ensemble Learning.\n",
    "\n",
    "Trainigsdatensatz erstellen z.B. durch geeignete Datentransformationen, Feature Engineering und ggf. Feature Extraction. Sequentielle Struktur der Daten soll berücksichtigt werden.\n",
    "Wenden Sie zur Modellerstellung (in dieser und den folgenden Aufgaben) geeignete Maßnahmen und Techniken an, damit die resultierenden Modelle eine moglichst hohe Gute aufweisen und beurteilen Sie diese anhand geeigneter Kriterien. Modularisieren und automatisieren Sie Ihren Workflow, damit die einzelnen Schritte in den folgenden Aufgaben ggf.\n",
    "wiederverwendet werden konnen. Achten Sie darauf, dass Ihre Modelle auf unbekannte Daten angewendet werden konnen, die ggf. fehlende Werte enthalten, auch wenn der gegebene Datensatz keine fehlenden Werte enthalt. Welche Features erweisen sich als besonders aussagekräftig für die gegebene Aufgabenstellung?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fd8e145",
   "metadata": {},
   "source": [
    "- **TP2** - Druck am Kompressor (bar).\n",
    "- **TP3** - An der Pneumatikzentrale erzeugter Druck (bar).\n",
    "- **H1** - Ventil, das aktiviert wird, wenn der vom Druckschalter der Steuerung abgelesene Druck über dem Betriebsdruck von 10,2 bar (bar) liegt.\n",
    "- **DV_pressure** - Druck, der durch den Druckabfall entsteht, wenn die Lufttrockentürme das Wasser. Wenn er gleich Null ist, arbeitet der Kompressor unter Last (bar).\n",
    "- **Reservoirs** - Druck in den auf den Zügen installierten Luftbehältern (bar).\n",
    "- **Oil_Temperature** - Temperatur des im Kompressor vorhandenen Öls (°C).\n",
    "- **Durchflussmesser** - Der Luftdurchfluss wurde an der pneumatischen Schalttafel gemessen (m^3\n",
    "/h).\n",
    "- **Motor_Current** - Strom des Motors, der die folgenden Werte aufweisen sollte: (i) nahe 0 A, wenn der (ii) nahe bei 4 A, wenn der Kompressor im Leerlauf arbeitet, und (iii) nahe bei 7 A, wenn der Kompressor der Kompressor unter Last arbeitet (A);\n",
    "- **COMP** - Elektrisches Signal des Lufteinlassventils des Kompressors. Es ist aktiv, wenn der Kompressor keine Luft ansaugt am Kompressor, d.h. der Kompressor schaltet ab oder arbeitet entlastet.\n",
    "- **DV_electric** - Elektrisches Signal, das das Auslassventil des Verdichters steuert. Wenn es aktiv ist, bedeutet es, dass der Kompressor unter Last arbeitet, wenn es nicht aktiv ist, bedeutet es, dass der Kompressor ausgeschaltet oder entlastet ist.\n",
    "- **TOWERS** - Signal, das festlegt, welcher Turm die Luft trocknet und welcher Turm die der Luft entzogene Feuchtigkeit abführt. Wenn es nicht aktiv ist, bedeutet es, dass Turm eins in Betrieb ist, wenn es aktiv ist, bedeutet es, fass Turm zwei in Betrieb ist.\n",
    "- **MPG** - Ist für die Aktivierung des Einlassventils verantwortlich, um den Kompressor unter Last zu starten, wenn der Druck in der der APU unter 8,2 bar liegt. Folglich aktiviert er den Sensor COMP, der das gleiche Verhalten wie der MPG-Sensor.\n",
    "- **LPS** - Signal aktiviert, wenn der Druck niedriger als 7 bar ist.\n",
    "- **Pressure_switch** - Signal, das aktiviert wird, wenn ein Druck am Vorsteuerventil festgestellt wird.\n",
    "- **Oil_Level** - Der Ölstand am Verdichter ist aktiv (gleich eins), wenn der Ölstand unter den erwarteten\n",
    "Werten liegt.\n",
    "- **Caudal_impulses** - Vom Durchflussmesser erzeugtes Signal, das den Luftdurchfluss pro Sekunde.\n",
    "\n",
    "\n",
    "Was die GPS-Informationen betrifft, so wurde der Zug mit einer sekundären GPS-Antenne ausgestattet, um Folgendes zu erfassen\n",
    "folgenden Daten:\n",
    "- **gpsLong** - Längengrad-Position (°).\n",
    "- **gpsLat** - Position des Breitengrades (°).\n",
    "- **gpsSpeed** - Geschwindigkeit (km/h).\n",
    "- **gpsQuality** - Signalqualität.\n",
    "\n",
    "!!!\n",
    "Bei der APU handelt es sich um eine Systemkomponente des Zugs, die im laufenden Betrieb ver-\n",
    "schiedene wichtige Funktionen erf ̈ullt, und deren Ausfall eine sofortige Außerbetriebnahme und\n",
    "Reparatur erforderlich macht. Weiterhin werden Angaben zu drei St ̈orungsf ̈allen gemacht, die\n",
    "sich w ̈ahrend des o.g. Betrachtungszeitraums ergeignet haben. Diese k ̈onnen verwendet werden,\n",
    "um geeignete Zielvariablen f ̈ur Methoden des Supervised Learning abzuleiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "df = pd.read_csv('dataset_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5a5b24",
   "metadata": {},
   "source": [
    "### Failures:\n",
    "- Failure 1: 28.02.2022 23:00:00 - 01.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dae378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A function that takes two timestamps as input and visulizes the data between them\n",
    "def plot_df(df, start, end):\n",
    "    # Create a new dataframe with the data between start and end timestamps\n",
    "    mask = (df['timestamp'] > start) & (df['timestamp'] <= end)\n",
    "    df_plot = df.loc[mask]\n",
    "    # Create a plotly figure\n",
    "    fig = px.line(df_plot, x='timestamp', y='LPS', title='LPS')\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "# A function that takes two timestamps as input and visulizes the data between them using matplotlib\n",
    "def plot_df_matplotlib(df : DataFrame, start, end, column, highlightStart: dt.datetime, highlightEnd: dt.datetime):\n",
    "    # Create a new dataframe with the data between start and end timestamps\n",
    "    mask = (df['timestamp'] > start) & (df['timestamp'] <= end)\n",
    "    df_plot = df.loc[mask]\n",
    "    # Create a matplotlib figure\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_plot['timestamp'], df_plot[column])\n",
    "    ax.set(xlabel='timestamp', ylabel=column, title=column)\n",
    "    #display 20 x-axis labels\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "    ax.grid()\n",
    "    formatter = DateFormatter('%H:%M:%S')\n",
    "    plt.axvspan(highlightStart, highlightEnd, color='red', alpha=0.5)\n",
    "    fig1 = plt.gcf()\n",
    "    fig1.axes[0].xaxis.set_major_formatter(formatter)  # Set the x-axis to display time\n",
    "    fig1.set_size_inches(18.5, 2.5)\n",
    "    #fig1.savefig('test2png.png', dpi=100)\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aus Tabelle\n",
    "err_1_start = dt.datetime(2022,2,28,21,53)\n",
    "err_1_end = dt.datetime(2022,3,1,2,00)\n",
    "err_2_start = dt.datetime(2022,3,23,14,54)\n",
    "err_2_end = dt.datetime(2022,3,23,15,24)\n",
    "err_3_start = dt.datetime(2022,5,30,12,00)\n",
    "err_3_end = dt.datetime(2022,6,2,6,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f850e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fehler 1\n",
    "start = dt.datetime(2022,2,28,21,30)\n",
    "end = dt.datetime(2022,3,1,6,45)\n",
    "# err_1_start = dt.datetime(2022,2,28,22,12)\n",
    "# err_1_end = dt.datetime(2022,3,1,6,27)\n",
    "\n",
    "plot_df_matplotlib(df, start, end, 'LPS', err_1_start, err_1_end)\n",
    "plot_df_matplotlib(df, start, end, 'TP3', err_1_start, err_1_end)\n",
    "plot_df_matplotlib(df, start, end, 'DV_pressure', err_1_start, err_1_end)\n",
    "plot_df_matplotlib(df, start, end, 'Oil_temperature', err_1_start, err_1_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fehler 2\n",
    "start = dt.datetime(2022,3,23,11,00)\n",
    "end = dt.datetime(2022,3,23,16,00)\n",
    "# err_2_start = dt.datetime(2022,3,24,11,15)\n",
    "# err_2_end = dt.datetime(2022,3,24,15,8)\n",
    "\n",
    "\n",
    "plot_df_matplotlib(df, start, end, 'LPS', err_2_start, err_2_end)\n",
    "plot_df_matplotlib(df, start, end, 'TP3', err_2_start, err_2_end)\n",
    "plot_df_matplotlib(df, start, end, 'DV_pressure', err_2_start, err_2_end)\n",
    "plot_df_matplotlib(df, start, end, 'Oil_temperature', err_2_start, err_2_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fehler 3\n",
    "start = dt.datetime(2022,5,30,4,00)\n",
    "end = dt.datetime(2022,6,2,15,00)\n",
    "# err_3_start = dt.datetime(2022,5,30,12,17)\n",
    "# err_3_end = dt.datetime(2022,6,2,7,40)\n",
    "\n",
    "plot_df_matplotlib(df, start, end, 'LPS', err_3_start, err_3_end)\n",
    "plot_df_matplotlib(df, start, end, 'TP3', err_3_start, err_3_end)\n",
    "plot_df_matplotlib(df, start, end, 'DV_pressure', err_3_start, err_3_end)\n",
    "plot_df_matplotlib(df, start, end, 'Oil_temperature', err_3_start, err_3_end)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91a2aaa6",
   "metadata": {},
   "source": [
    "### Zeiterfassung der Störungsfälle (für eigene Labels)\n",
    "**Fall 1:**\n",
    "- 28.2-1.03: 22:12:00 - 6:27:00\n",
    "\n",
    "**Fall 2:**\n",
    "- 24.03: 11:15:00 - 15:08:00\n",
    "\n",
    "**Fall 3:**\n",
    "- 30.05-02.06: 04:00:00 - 07:40:00 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c711a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add another column to the dataframe that is called Label is 1 on all rows where the timestamp is between err_1_start and err_1_end or err_2_start and err_2_end or err_3_start and err_3_end and 0 otherwise\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) | \n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4489d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that plots every column in a dataframe in a single plot using matplotlib in subplots\n",
    "def plot_df_all_columns(df, start, end):\n",
    "    # Create a new dataframe with the data between start and end timestamps\n",
    "    mask = (df['timestamp'] > start) & (df['timestamp'] <= end)\n",
    "    df_plot = df.loc[mask]\n",
    "    # Create a matplotlib figure\n",
    "    fig, axs = plt.subplots(df_plot.columns.size, 1, figsize=(20, 40))\n",
    "\n",
    "    i = 0\n",
    "    for col in df_plot.columns:\n",
    "        axs[i].plot(df_plot['timestamp'], df_plot[col])\n",
    "        axs[i].set(xlabel='timestamp', ylabel=col, title=col)\n",
    "        #display 20 x-axis labels\n",
    "        axs[i].xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "        axs[i].grid()\n",
    "        formatter = DateFormatter('%H:%M:%S')\n",
    "        fig1 = plt.gcf()\n",
    "        fig1.axes[0].xaxis.set_major_formatter(formatter)\n",
    "        i = i + 1\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d36a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = err_1_start - dt.timedelta(hours=3*4, seconds=100)\n",
    "e = err_1_end   + dt.timedelta(hours=3*4)\n",
    "plot_df_all_columns(df, s, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Klassifizierung mit Decision Tree\n",
    "class DecisionTreeBinaryClassifier:\n",
    "    def __init__(self, test_size=0.3, random_state=42, max_depth=7, min_samples_leaf=10, columns=[]):\n",
    "        #set parameters\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.columns = columns\n",
    "        self.model = DecisionTreeClassifier(max_depth = self.max_depth, min_samples_leaf = self.min_samples_leaf)\n",
    "\n",
    "    def fit(self, df, label):\n",
    "        #split data into train and test\n",
    "        train_n, test_n = train_test_split(df, test_size=self.test_size, random_state=self.random_state)\n",
    "        #split train and test into X and y\n",
    "        y_train = train_n[label]\n",
    "        y_test  = test_n[label]\n",
    "        #drop label from X\n",
    "        X_train = train_n.drop([label],axis=1)\n",
    "        X_test = test_n.drop([label],axis=1)\n",
    "        #keep only columns that are in self.columns\n",
    "        X_train = X_train[self.columns]\n",
    "        X_test = X_test[self.columns]\n",
    "        #save X and y for later use\n",
    "        self.X_test, self.X_train = X_test, X_train\n",
    "        self.y_test, self.y_train = y_test, y_train\n",
    "        #fit model\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def score(self):\n",
    "        #return accuracy score\n",
    "        return self.model.score(self.X_test, self.y_test)\n",
    "\n",
    "    def predict(self):\n",
    "        #return prediction\n",
    "        return self.model.predict(self.X_test)\n",
    "\n",
    "\n",
    "    def confusionMatrix(self):\n",
    "        y_pred = self.predict()\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.model.classes_)\n",
    "        return disp.plot()\n",
    "\n",
    "    def corellationMatrix(self):\n",
    "        corr = self.X_train.corr()\n",
    "        return sns.heatmap(corr,\n",
    "                xticklabels=corr.columns,\n",
    "                yticklabels=corr.columns)\n",
    "    \n",
    "    def decisionTreeGraph(self):\n",
    "        dot_data = tree.export_graphviz(self.model, out_file=None) \n",
    "        dot_data = tree.export_graphviz(self.model, out_file=None, \n",
    "                            feature_names=self.X_train.columns,  \n",
    "                            class_names=['0','1'],  \n",
    "                            filled=True, rounded=True,  \n",
    "                            special_characters=True) \n",
    "        graph = graphviz.Source(dot_data)\n",
    "        return graph\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84658daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13641fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02482218",
   "metadata": {},
   "outputs": [],
   "source": [
    "analog_sensors = ['TP2', 'TP3', 'H1', 'DV_pressure', 'Reservoirs', 'Oil_Temperature', 'Flowmeter', 'Motor_Current']\n",
    "digitial_sensors = ['COMP', 'DV_eletric', 'Towers', 'MPG', 'LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127cdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class MetroBinaryClassifier:\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, use_shortened: False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=7, min_samples_leaf=10, excluded_columns=[]):\n",
    "        self.n_components = n_components\n",
    "        self.windowSize = window_size\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.excluded_columns = excluded_columns\n",
    "        self.use_shortened = use_shortened\n",
    "\n",
    "        self.scaler = None\n",
    "        self.pca = None\n",
    "\n",
    "        self.feature_columns = None\n",
    "\n",
    "        #Dataset:\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "        #classifiers\n",
    "        self.decision_tree = None\n",
    "        self.random_forest = None\n",
    "        self.linear_regression = None\n",
    "        self.linear_SVM = None\n",
    "        self.naive_bayes = None\n",
    "        self.linear_discriminant_analysis = None\n",
    "        self.logistic_regression = None\n",
    "\n",
    "        #regressions\n",
    "        self.polyreg_scaled = None\n",
    "\n",
    "    def sliding_window(self, data: DataFrame, window_size = 60, label_func = None, excluded_columns = ['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality']):\n",
    "        \"\"\"\n",
    "        Creates a sliding window of size window_size for each column in data.\n",
    "        Performs the following operations on each column:\n",
    "        - mean\n",
    "        - std\n",
    "        - min\n",
    "        - max\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : DataFrame\n",
    "            The DataFrame that should be transformed.\n",
    "\n",
    "        window_size : int, optional\n",
    "            The size of the sliding window. The default is 60.\n",
    "\n",
    "        excluded_columns : list, optional\n",
    "            A list of columns that should be excluded from the sliding window. The default is ['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_rolling : DataFrame\n",
    "            The transformed DataFrame.\n",
    "        \"\"\"\n",
    "        columns = data.columns\n",
    "        columns  = [x for x in columns if x not in excluded_columns]\n",
    "        operations = ['mean', 'std', 'min', 'max', 'var']\n",
    "\n",
    "        #generate a dict, where the keys are the column names and the values are the operations that should be performed on the column\n",
    "        operations_dict = {}\n",
    "        for column in columns:\n",
    "            operations_dict[column] = operations\n",
    "\n",
    "        def mostFrequent(row):\n",
    "            return Counter(row).most_common(1)[0][0]\n",
    "\n",
    "        if(label_func == None):\n",
    "            operations_dict['Label'] = [mostFrequent]\n",
    "        else:\n",
    "            operations_dict['Label'] = [label_func]\n",
    "\n",
    "        df_rolling = data.rolling(window_size,center=True,step=window_size).agg(operations_dict)\n",
    "\n",
    "        #drop all columns that have NaN values\n",
    "        df_rolling = df_rolling.dropna()\n",
    "\n",
    "        #flatten df_rolling\n",
    "        df_rolling.columns = ['_'.join(col) for col in df_rolling.columns]\n",
    "\n",
    "        #rename Label_<opaeration> to Label\n",
    "        if(label_func == None):\n",
    "            df_rolling = df_rolling.rename(columns={'Label_mostFrequent': 'Label'})\n",
    "        else:\n",
    "            name = \"Label_\"\n",
    "            #check if the label_func is a string\n",
    "            if(isinstance(label_func, str)):\n",
    "                name += label_func\n",
    "\n",
    "            #check if the label_func is a function\n",
    "            elif(callable(label_func)):\n",
    "                name += label_func.__name__\n",
    "\n",
    "            #if the label_func is neither a string nor a function, raise an exception\n",
    "            else:\n",
    "                raise Exception(\"label_func is not a string or a function\")\n",
    "\n",
    "            df_rolling = df_rolling.rename(columns={name: 'Label'})\n",
    "        \n",
    "        return df_rolling\n",
    "    \n",
    "    def scale(self, train, test):\n",
    "        \"\"\"\n",
    "        Scale the data using StandardScaler\n",
    "        Fits the scaler on the training data and transforms the training and test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train : DataFrame\n",
    "            The training data.\n",
    "\n",
    "        test : DataFrame\n",
    "            The test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_rescaled : DataFrame\n",
    "            The rescaled training data.\n",
    "        \"\"\"\n",
    "        self.scaler = MinMaxScaler()\n",
    "        train_rescaled = self.scaler.fit_transform(train)\n",
    "        test_rescaled = self.scaler.transform(test)\n",
    "\n",
    "        return train_rescaled, test_rescaled\n",
    "    \n",
    "    def performPca(self, train, test):\n",
    "        \"\"\"\n",
    "        Perform PCA on the data.\n",
    "        Fits the PCA on the training data and transforms the training and test data.\n",
    "        This is done to reduce the dimensionality of the data.\n",
    "        This will slightly reduce the accuracy of the model, but will increase the speed of the model.\n",
    "        Uses the n_components parameter given by in the constructor of the class to determine the number of components.\n",
    "        (if a float is given, the number of components is determined by the explained variance)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train : DataFrame\n",
    "            The training data.\n",
    "\n",
    "        test : DataFrame\n",
    "            The test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_pca : DataFrame\n",
    "            The PCA transformed training data.\n",
    "\n",
    "        test_pca : DataFrame\n",
    "            The PCA transformed test data.\n",
    "        \"\"\"\n",
    "        self.pca = PCA(n_components=self.n_components,random_state=self.random_state)\n",
    "        train_pca = self.pca.fit_transform(train)\n",
    "        test_pca = self.pca.transform(test)\n",
    "\n",
    "        return train_pca, test_pca\n",
    "\n",
    "    \n",
    "    def prepareData(self, dataFrame, do_scale=True, do_pca=True,do_sliding=True, label_func=None):\n",
    "        \"\"\"\n",
    "        Prepare the data for the model.\n",
    "        This includes(if desired):\n",
    "        - sliding window\n",
    "        - scaling\n",
    "        - PCA\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataFrame : DataFrame\n",
    "            The DataFrame that should be prepared.\n",
    "\n",
    "        do_scale : bool, optional\n",
    "            Whether the data should be scaled. The default is True.\n",
    "\n",
    "        do_pca : bool, optional\n",
    "            Whether PCA should be performed on the data. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train : DataFrame\n",
    "            The training data.\n",
    "\n",
    "        X_test : DataFrame\n",
    "            The test data.\n",
    "\n",
    "        y_train : DataFrame\n",
    "            The training labels.\n",
    "\n",
    "        y_test : DataFrame\n",
    "            The test labels.\n",
    "        \"\"\"\n",
    "\n",
    "        data = dataFrame\n",
    "\n",
    "        if(self.use_shortened):\n",
    "            data = self.shorten(dataFrame)\n",
    "\n",
    "        if(do_sliding):\n",
    "            data_prepared = self.sliding_window(data, window_size=self.windowSize, excluded_columns=self.excluded_columns,label_func=label_func)\n",
    "        else:\n",
    "            data_prepared = data\n",
    "\n",
    "        y = data_prepared['Label']\n",
    "\n",
    "        if(not do_sliding):\n",
    "            ex = self.excluded_columns\n",
    "            ex.remove('Label')\n",
    "            data_prepared = data_prepared.drop(columns=ex, axis=1)\n",
    "            \n",
    "        data_prepared.drop(['Label'], axis=1, inplace=True)\n",
    "\n",
    "        X = data_prepared\n",
    "\n",
    "        self.feature_columns = X.columns\n",
    "\n",
    "        print(self.feature_columns)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "\n",
    "        \n",
    "        if(do_scale):\n",
    "            X_train, X_test = self.scale(X_train, X_test)\n",
    "\n",
    "        if(do_pca):\n",
    "            X_train, X_test = self.performPca(X_train, X_test)\n",
    "\n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    \n",
    "    def evalModels(self):\n",
    "        \"\"\"\n",
    "        Evaluate the models.\n",
    "        Prints a Plotly interactive Bar chart for each fitted model that shows the precision, recall and f1 score for each class(only '1' and '0').\n",
    "        \"\"\"\n",
    "\n",
    "        calssifiers = [self.decision_tree, self.random_forest, self.linear_SVM, self.naive_bayes, self.linear_discriminant_analysis, self.logistic_regression]\n",
    "        regression = self.linear_regression\n",
    "\n",
    "\n",
    "        results = []\n",
    "        clfs = []\n",
    "        # precison = {\n",
    "        #     '0': [],\n",
    "        #     '1': []\n",
    "        # }\n",
    "        # recall = {\n",
    "        #     '0': [],\n",
    "        #     '1': []\n",
    "        # }\n",
    "        # f1_score = {\n",
    "        #     '0': [],\n",
    "        #     '1': []\n",
    "        # }\n",
    "\n",
    "        precison = {}\n",
    "        recall = {}\n",
    "        f1_score = {}\n",
    "\n",
    "\n",
    "        for label in self.y_test.unique():\n",
    "            precison[str(label)] = []\n",
    "            recall  [str(label)] = []\n",
    "            f1_score[str(label)] = []\n",
    "\n",
    "\n",
    "        for classifier in calssifiers:\n",
    "            if(classifier is not None):\n",
    "                y_true, y_pred = self.y_test, classifier.predict(self.X_test)\n",
    "                evaluation = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "                clfs.append(classifier.__class__.__name__)\n",
    "                    \n",
    "                for label in self.y_test.unique():\n",
    "\n",
    "                    precison[str(label)].append(evaluation[str(label)]['precision'])\n",
    "                    recall  [str(label)].append(evaluation[str(label)]['recall'])\n",
    "                    f1_score[str(label)].append(evaluation[str(label)]['f1-score'])\n",
    "\n",
    "        print(precison, recall, f1_score)\n",
    "\n",
    "        print(results)\n",
    "        #make a plot with plotly, where on the x axis are  the classifiers, each classifier has bar for classes precison, recall and f1-score\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for label in self.y_test.unique():\n",
    "            data.append(go.Bar(name ='[' + str(label) + '] precision',    x=clfs, y=precison[str(label)] ,text=precison[str(label)] ,textposition='auto',))\n",
    "            data.append(go.Bar(name ='[' + str(label) + '] recall',       x=clfs, y=recall[str(label)]   ,text=recall[str(label)]   ,textposition='auto',))\n",
    "            data.append(go.Bar(name ='[' + str(label) + '] f1-score',     x=clfs, y=f1_score[str(label)] ,text=f1_score[str(label)] ,textposition='auto',))\n",
    "\n",
    "        fig = go.Figure(data=data)\n",
    "        # Change the bar mode\n",
    "        fig.update_layout(barmode='group')\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    def ConfusionMatricesis(self):\n",
    "        \"\"\"\n",
    "        Shows a plotly interactive confusion matrix for each fitted model.\n",
    "        \"\"\"\n",
    "\n",
    "        calssifiers = [self.decision_tree, self.random_forest, self.linear_SVM, self.naive_bayes, self.linear_discriminant_analysis, self.logistic_regression]\n",
    "\n",
    "        #list of all classifiers that are not None\n",
    "        clfs = [clf for clf in calssifiers if clf is not None]\n",
    "        y_true = self.y_test\n",
    "        cmats = []\n",
    "        \n",
    "        #get the confusion matrix for each classifier\n",
    "        for clf in clfs:\n",
    "            y_pred = clf.predict(self.X_test)\n",
    "            cmats.append(confusion_matrix(y_true, y_pred)) \n",
    "            \n",
    "        #Titles for each matrix is the class name of the classifier\n",
    "        titles = [clf.__class__.__name__ for clf in clfs]\n",
    "\n",
    "        #Calculate the optimal grid size for the plot\n",
    "        n = len(clfs)\n",
    "        rows = int(math.sqrt(n))\n",
    "        cols = int(math.ceil(n / rows))\n",
    "        \n",
    "        #plotgird for the confusion matrices\n",
    "        fig = make_subplots(rows=rows, cols=cols,\n",
    "                        subplot_titles=(titles))\n",
    "\n",
    "        \n",
    "        #add the confusion matrices to the plot\n",
    "        r=1\n",
    "        c=1\n",
    "        for i in range(len(clfs)):\n",
    "            heatmap = go.Heatmap(\n",
    "                z=cmats[i],\n",
    "                text=cmats[i],\n",
    "                texttemplate=\"%{text}\",\n",
    "                textfont={\"size\": 10},)            \n",
    "\n",
    "            fig.add_trace(heatmap,row=r,col=c)\n",
    "            \n",
    "            #fill the grid from left to right, top to bottom\n",
    "            if(c<cols):\n",
    "                c=c+1\n",
    "            else:\n",
    "                r=r+1\n",
    "                c=1\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    def fitDecisionTree(self, max_depth, min_samples_leaf, min_samples_split):\n",
    "        \"\"\"\n",
    "        Fit a Decision Tree Classifier to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_depth : int\n",
    "            How deep the tree search should go.\n",
    "\n",
    "        min_samples_leaf : int\n",
    "            The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "        do_min_samples_split : int\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        \"\"\"\n",
    "        self.decision_tree = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split)\n",
    "        self.decision_tree.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def decisionTreeConfusionMatrix(self):\n",
    "        \"\"\"\n",
    "        Plot a confusion matrix for the Decision Tree Classifier.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        disp : ConfusionMatrixDisplay\n",
    "            Dispaly of the confusion matrix.\n",
    "        \"\"\"\n",
    "        y_pred = self.decision_tree.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.decision_tree.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def decisionTreeGraph(self):\n",
    "        dot_data = tree.export_graphviz(self.decision_tree, out_file=None) \n",
    "        dot_data = tree.export_graphviz(self.decision_tree, out_file=None, \n",
    "                            feature_names=self.X_train.columns,  \n",
    "                            class_names=['0','1'],  \n",
    "                            filled=True, rounded=True,  \n",
    "                            special_characters=True) \n",
    "\n",
    "        graph = graphviz.Source(dot_data)\n",
    "        return graph\n",
    "    \n",
    "    def fitRandomForest(self, n_estimators, max_depth, min_samples_leaf, min_samples_split):\n",
    "        \"\"\"\n",
    "        Fit a Random Forest Classifier to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int\n",
    "            The number of trees in the forest.\n",
    "        \n",
    "        max_depth : int\n",
    "            How deep the tree search should go.\n",
    "        \n",
    "        min_samples_leaf : int\n",
    "            The minimum number of samples required to be at a leaf node.\n",
    "        \n",
    "        min_samples_split : int\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        \"\"\"\n",
    "        self.random_forest = RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split)\n",
    "        self.random_forest.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def randomForestConfusionMatrix(self):\n",
    "        \"\"\"\n",
    "        Plot a confusion matrix for the Random Forest Classifier.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        disp : ConfusionMatrixDisplay\n",
    "            Dispaly of the confusion matrix.\n",
    "        \"\"\"\n",
    "        y_pred = self.random_forest.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.random_forest.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "\n",
    "    def getPredictScore(self):\n",
    "        \"\"\"\n",
    "        Get the prediction from the classifier.\n",
    "        \"\"\"\n",
    "        # get predict from all classifiers that are not None\n",
    "        classifiers = [self.decision_tree, self.random_forest, self.logistic_regression]\n",
    "\n",
    "        for classifier in classifiers:\n",
    "            if classifier is not None:\n",
    "                # make prediction using the classifier and X_test\n",
    "                predict = classifier.predict(self.X_test)\n",
    "\n",
    "                # print classifier name with score\n",
    "                score = classifier.score(self.X_test, self.y_test)\n",
    "                print(classifier.__class__.__name__,\"\\nPredict: \", predict, \"\\nScore:\", score, '\\n')\n",
    "\n",
    "                if classifier.__class__.__name__ == 'DecisionTreeClassifier':\n",
    "                    self.decision_predict = predict\n",
    "                elif classifier.__class__.__name__ == 'RandomForestClassifier':\n",
    "                    self.random_predict = predict\n",
    "                elif classifier.__class__.__name__ == 'LogisticRegression':\n",
    "                    self.logistic_predict = predict\n",
    "        \n",
    "\n",
    "    def fitLinearRegression(self):\n",
    "        self.linear_regression = LinearRegression(n_jobs=-1)\n",
    "        self.linear_regression.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def linearRegressionConfusionMatrix(self):\n",
    "        y_pred = self.linear_regression.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.linear_regression.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def fitLinearSVM(self):\n",
    "        self.linear_SVM = LinearSVC()\n",
    "        self.linear_SVM.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def fitNaiveBayes(self):\n",
    "        self.naive_bayes = GaussianNB(var_smoothing=1e-15,)\n",
    "        self.naive_bayes.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def naiveBayesConfusionMatrix(self):\n",
    "        y_pred = self.naive_bayes.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.naive_bayes.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def fitLinearDiscriminantAnalysis(self):\n",
    "        self.linear_discriminant_analysis = LinearDiscriminantAnalysis()\n",
    "        self.linear_discriminant_analysis.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def linearDiscriminantAnalysisConfusionMatrix(self):\n",
    "        y_pred = self.linear_discriminant_analysis.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.linear_discriminant_analysis.classes_)\n",
    "        return disp.plot()\n",
    "\n",
    "    def fitLogisticRegression(self, C, solver, max_iter, class_weight):\n",
    "        \"\"\"\n",
    "        Fit a Logistic Regression Classifier to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        C : int\n",
    "            Strength of the regularization.\n",
    "\n",
    "        solver : int\n",
    "            Algorithm to use in the optimization problem.\n",
    "\n",
    "        max_iter : int\n",
    "            How many iterations to run the solver for.\n",
    "\n",
    "        class_weight : str\n",
    "            Weights associated with classes.\n",
    "        \"\"\"\n",
    "        self.logistic_regression = LogisticRegression(n_jobs=-1, C=C, solver=solver, max_iter=max_iter, class_weight=class_weight)\n",
    "        self.logistic_regression.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def logisticRegressionConfusionMatrix(self):\n",
    "        \"\"\"\n",
    "        Plot a confusion matrix for the Logistic Regression Classifier.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        disp : ConfusionMatrixDisplay\n",
    "            Dispaly of the confusion matrix.\n",
    "        \"\"\"\n",
    "        y_pred = self.logistic_regression.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.logistic_regression.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def linearDiscriminantAnalysisFindSolver(self):\n",
    "        # define model\n",
    "        model = LinearDiscriminantAnalysis()\n",
    "        # define model evaluation method\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "        # define grid\n",
    "        grid = dict()\n",
    "        grid['solver'] = ['svd', 'lsqr', 'eigen']\n",
    "        # define search\n",
    "        search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        # perform the search\n",
    "        results = search.fit(self.X_train, self.y_train)\n",
    "        # summarize\n",
    "        print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "        print('Config: %s' % results.best_params_)\n",
    "    \n",
    "    def linearSVMConfusionMatrix(self):\n",
    "        y_pred = self.linear_SVM.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.linear_SVM.classes_)\n",
    "        return disp.plot()\n",
    "\n",
    "    def fitPlynomialRegression(self,degree=9):\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        self.polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "        self.polyreg_scaled.fit(self.X_train,self.y_train)\n",
    "\n",
    "    def polyregScore(self):\n",
    "        return self.polyreg_scaled.score(self.X_test, self.y_test)    \n",
    "        \n",
    "    def shorten(self, df):\n",
    "        \"\"\"\n",
    "        Shorten the dataframe around the failures. This should be done to test effects of changes on the models, but shpouldnt be done for the final model.\n",
    "        \"\"\"\n",
    "        dfMin = df.copy()\n",
    "        #drop vor Fail 1\n",
    "        dfMin.drop(dfMin[dfMin['timestamp'] < '2022-02-26 23:00:00'].index,inplace=True)\n",
    "\n",
    "        #drop zwischen Fail1 und Fail2\n",
    "        dfMin.drop(dfMin[(dfMin['timestamp'] > '2022-03-03 00:00:00') & (dfMin['timestamp'] <= '2022-03-21 00:00:00')].index,inplace=True)\n",
    "\n",
    "        #drop zwischen Fail 2 und Fail 3\n",
    "        dfMin.drop(dfMin[(dfMin['timestamp'] > '2022-03-25 00:00:00') & (dfMin['timestamp'] <= '2022-05-28 00:00:00')].index,inplace=True)\n",
    "\n",
    "        dfMin['Label'].value_counts()\n",
    "        \n",
    "        return dfMin\n",
    "    \n",
    "\n",
    "    def lossFunctionDecisionTree2(self):\n",
    "        \"\"\"\n",
    "        Returns the loss function for the decision tree.\n",
    "        \"\"\"\n",
    "        # Entscheidungsbaum erstellen\n",
    "        clf_e = DecisionTreeClassifier(criterion='entropy')\n",
    "        clf_e = clf_e.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Entropiewert berechnen\n",
    "        entropy_score = clf_e.score(self.X_test, self.y_test)\n",
    "        print(\"Entropie-Wert: \", entropy_score)\n",
    "\n",
    "        # Gini-Wert berechnen\n",
    "        clf = DecisionTreeClassifier(criterion='gini', random_state=self.random_state, max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf)\n",
    "        clf = clf.fit(self.X_test, self.y_test)\n",
    "        gini_score = clf.score(self.X_test, self.y_test)\n",
    "        print(\"Gini-Wert: \", gini_score)\n",
    "\n",
    "    def hyperparamsDecisionTree(self):\n",
    "        \"\"\"\n",
    "        Returns the recall and the best parameters for the decision tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        recall : float\n",
    "            The recall of the model.\n",
    "\n",
    "        best_params : dict\n",
    "            The best parameters for the model.\n",
    "        \"\"\"\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {'max_depth': [None], \n",
    "                    'min_samples_leaf': [*range(2,6)], \n",
    "                    'min_samples_split': [*range(2,6)]}\n",
    "\n",
    "        clf = tree.DecisionTreeClassifier() \n",
    "        recall, best_params = self.hyperparams(param_grid, clf)\n",
    "        return recall, best_params\n",
    "\n",
    "    def hyperparamsRandomForest(self):\n",
    "        \"\"\"\n",
    "        Returns the recall and the best parameters for the random forest.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        recall : float\n",
    "            The recall of the model.\n",
    "\n",
    "        best_params : dict\n",
    "            The best parameters for the model.\n",
    "        \"\"\"\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {'n_estimators': [10], \n",
    "                    'max_depth': [None], \n",
    "                    'min_samples_leaf': [*range(1, 6)], \n",
    "                    'min_samples_split': [*range(2, 6)]}\n",
    "\n",
    "        rf = RandomForestClassifier()\n",
    "        recall, best_params = self.hyperparams(param_grid, rf)\n",
    "        return recall, best_params\n",
    "\n",
    "    def hyperparamsLogisticRegression(self):\n",
    "        \"\"\"\n",
    "        Returns the recall and the best parameters for the logistic regression.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        recall : float\n",
    "            The recall of the model.\n",
    "\n",
    "        best_params : dict\n",
    "            The best parameters for the model.\n",
    "        \"\"\"\n",
    "        # Define the hyperparameter grid\n",
    "        param_grid = {'C': [0.001, 0.01, 0.1,],\n",
    "                    'solver': ['saga', 'lbfgs'],\n",
    "                    'max_iter': [1000],\n",
    "                    'class_weight': [None, 'balanced']}\n",
    "\n",
    "        lg = LogisticRegression()\n",
    "        recall, best_params = self.hyperparams(param_grid, lg)\n",
    "        return recall, best_params\n",
    "    \n",
    "    def hyperparams(self, param_grid, model):\n",
    "        \"\"\"\n",
    "        Returns the accuracy and the best parameters for a model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param_grid : dict \n",
    "            Dictionary with the hyperparameters.\n",
    "\n",
    "        model : model\n",
    "            The model to be used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy : float\n",
    "            The accuracy of the model.\n",
    "\n",
    "        grid_search.best_params_ : dict\n",
    "            The best parameters for the model.\n",
    "        \"\"\"\n",
    "        # Create a custom scorer function that returns the recall score for label 1\n",
    "        def recall_scorer(y_true, y_pred):\n",
    "            return recall_score(y_true, y_pred, pos_label=1)\n",
    "            \n",
    "        # Define the scorer\n",
    "        scorer = make_scorer(recall_scorer)\n",
    "\n",
    "        # Define the GridSearchCV object\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring=scorer, n_jobs=-1, error_score=np.nan, verbose=1)\n",
    "\n",
    "        # Fit the GridSearchCV object to the training data\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Predict using the best model found by GridSearchCV\n",
    "        y_pred = grid_search.predict(self.X_test)\n",
    "\n",
    "        # Calculate the recall score\n",
    "        recall = recall_score(self.y_test, y_pred)\n",
    "        return recall, grid_search.best_params_   \n",
    "\n",
    "    def plotFeatureImportances(self, feature_columns_index):\n",
    "        \"\"\"\n",
    "        Plots the feature importances of the models.S\n",
    "        \"\"\"\n",
    "        model_name_random = 'RandomForestClassifier'\n",
    "        model_name_decision = 'DecisionTreeClassifier'\n",
    "        model_name_logistic = 'LogisticRegression'\n",
    "\n",
    "        models = [\n",
    "            (self.random_forest, model_name_random),\n",
    "            (self.decision_tree, model_name_decision),\n",
    "            (self.logistic_regression, model_name_logistic),\n",
    "        ]\n",
    "    \n",
    "        for model, name in models:\n",
    "            if model is None:\n",
    "                continue\n",
    "            importances = None\n",
    "            title = None\n",
    "            if name == model_name_random:\n",
    "                importances = model.feature_importances_\n",
    "                #Certain Feature Columns should be used\n",
    "                if feature_columns_index == True:\n",
    "                    self.random_importances = pd.Series(importances, index=self.feature_columns)\n",
    "                #All Feature Columns should be used\n",
    "                else:\n",
    "                    self.random_importances = pd.Series(importances, index=model.feature_names_in_)\n",
    "                self.random_importances = self.random_importances.sort_values(ascending=False)\n",
    "                std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "                title = f\"Feature importances using MDI - {name}\"\n",
    "                self.__plottingFeatureImportances(model_importances=self.random_importances, model_name_random=model_name_random, std=std, name=name, title=title)\n",
    "            elif name == model_name_decision:\n",
    "                importances = model.feature_importances_\n",
    "                #Certain Feature Columns should be used\n",
    "                if feature_columns_index == True:\n",
    "                    self.decision_importances = pd.Series(importances, index=self.feature_columns)\n",
    "                #All Feature Columns should be used\n",
    "                else:\n",
    "                    self.decision_importances = pd.Series(importances, index=model.feature_names_in_)\n",
    "                self.decision_importances = self.decision_importances.sort_values(ascending=False)\n",
    "                title = f\"Feature importances using MDI - {name}\"\n",
    "                self.__plottingFeatureImportances(model_importances=self.decision_importances, model_name_random=model_name_random, std=std, name=name, title=title)\n",
    "            elif name == model_name_logistic:\n",
    "                importances = np.abs(model.coef_[0])\n",
    "                #Certain Feature Columns should be used\n",
    "                if feature_columns_index == True:\n",
    "                    self.logistic_importances = pd.Series(importances, index=self.feature_columns)\n",
    "                #All Feature Columns should be used\n",
    "                else:\n",
    "                    self.logistic_importances = pd.Series(importances, index=model.feature_names_in_)\n",
    "                self.logistic_importances = self.logistic_importances.sort_values(ascending=False)\n",
    "                title = f\"Feature importances using MDI - {name}\"\n",
    "                self.__plottingFeatureImportances(model_importances=self.logistic_importances, model_name_random=model_name_random, std=std, name=name, title=title)\n",
    "\n",
    "\n",
    "    def __plottingFeatureImportances(self, model_importances, model_name_random, std, name, title):\n",
    "            \"\"\"\n",
    "            Plots the feature importances of the models.\n",
    "            \"\"\"\n",
    "            FONT_SIZE_HEADLINE = 30\n",
    "            FONT_SIZE_LABEL = 20\n",
    "            fig, ax = plt.subplots(figsize=(40, 15))\n",
    "            model_importances.plot.bar(yerr=std if name == model_name_random else None, ax=ax)\n",
    "            ax.set_title(title, fontsize=FONT_SIZE_HEADLINE)\n",
    "            ax.set_ylabel(\"Mean decrease in impurity\", fontsize=FONT_SIZE_HEADLINE)\n",
    "            ax.tick_params(axis='x', which='major', labelsize=FONT_SIZE_LABEL)\n",
    "            ax.tick_params(axis='y', which='major', labelsize=FONT_SIZE_LABEL)\n",
    "            fig.tight_layout()\n",
    "\n",
    "    \n",
    "    def mostImportantFeatures(self, index):\n",
    "        \"\"\"\n",
    "        Prints the 10 most important features of the models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            The number of features to print.\n",
    "        \"\"\"\n",
    "        model_name_random = 'RandomForest Classifier'\n",
    "        model_name_decision = 'DecisionTree Classifier'\n",
    "        model_name_logistic = 'Logistic Regression'\n",
    "\n",
    "        models = [\n",
    "            (self.random_importances, model_name_random),\n",
    "            (self.decision_importances, model_name_decision),\n",
    "            (self.logistic_importances, model_name_logistic),\n",
    "        ]\n",
    "\n",
    "        #Create a DataFrame with the most important features in one table\n",
    "        result_df = pd.DataFrame()\n",
    "        for model, name in models:\n",
    "            if model is None:\n",
    "                continue\n",
    "            most_important = model.sort_values(ascending=False).head(index)\n",
    "            most_important = most_important.reset_index()\n",
    "            most_important.columns = [f'{name}_Feature', f'{name}_Importance']\n",
    "            result_df = result_df.merge(most_important, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        return result_df\n",
    "    \n",
    "    def plotSingleConfusionMatrix(self, model):\n",
    "        \"\"\"\n",
    "        Plots the confusion matrix of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : model\n",
    "            The model to plot.\n",
    "        \"\"\"\n",
    "        # Predict using the best model found by GridSearchCV\n",
    "        y_pred = model.predict(self.X_test)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        cm.shape\n",
    "        #remove the 0 columns from the confusion matrix(because the majority of the data is 0)\n",
    "        cm = cm[:,1:]\n",
    "        #plot the confusion matrix\n",
    "        plt.figure(figsize=(20,14))\n",
    "        sns.heatmap(cm, annot=False)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Truth')\n",
    "        plt.show()\n",
    "\n",
    "    def plotSingleActualPredicted(self, model):\n",
    "        \"\"\"\n",
    "        Plots the actual and predicted values of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : model\n",
    "            The model to plot.\n",
    "        \"\"\"\n",
    "        # Predict using the best model found by GridSearchCV\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        df_res = pd.DataFrame({'Actual': self.y_test, 'Predicted': y_pred})\n",
    "        #sort by index\n",
    "        df_res.sort_index(inplace=True)\n",
    "        #plot with plotly\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Actual'], name='Actual'))\n",
    "        fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Predicted'], name='Predicted'))\n",
    "        fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a09e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_accuracy = 0\n",
    "# #loop though window_size range from 300-700\n",
    "# for window_size in range(300,500,10):\n",
    "#     mb = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=window_size, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "#     #print(\"Prepareing Data...\")\n",
    "#     _, _, _, _ = mb.prepareData(df, do_scale=True, do_pca=False)\n",
    "#     accuracy, grid_search_best_params = mb.hyperparamsDecisionTree()\n",
    "#     if accuracy > best_accuracy:\n",
    "#              best_window_size = window_size\n",
    "#              best_accuracy = accuracy\n",
    "#              best_params = grid_search_best_params\n",
    "\n",
    "# print(\"Best Accuracy: \", best_accuracy)\n",
    "# print(\"Best Params: \", best_params)\n",
    "# print( \"Best Window Size: \", best_window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db96a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start\")\n",
    "mb = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=440, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "print(\"Prepareing Data...\")\n",
    "_, _, _, _ = mb.prepareData(df, do_scale=True, do_sliding=False, do_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Search best hyperparameters for the models\n",
    "# recall_decision, grid_search_best_params_decision = mb.hyperparamsDecisionTree()\n",
    "# print(\"Best recall Decision Tree: \", recall_decision)\n",
    "# print(\"Best Params Decision Tree: \", grid_search_best_params_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69116ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall_logistic, grid_search_best_params_logistic = mb.hyperparamsLogisticRegression()\n",
    "# print(\"Best recall Logistic Regression: \", recall_logistic)\n",
    "# print(\"Best Params Logistic Regression: \", grid_search_best_params_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d410c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall_random, grid_search_best_params_random = mb.hyperparamsRandomForest()\n",
    "# print(\"Best recall Random Forest: \", recall_random)\n",
    "# print(\"Best Params Random Forest: \", grid_search_best_params_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the models with the best parameters\n",
    "print(\"Fitting Decision Tree...\")\n",
    "mb.fitDecisionTree(max_depth=None, min_samples_leaf=1, min_samples_split=2)\n",
    "print(\"Fitting Random Forest...\")\n",
    "mb.fitRandomForest(n_estimators=10, max_depth=None, min_samples_leaf=1, min_samples_split=3)\n",
    "print(\"Fitting Logistic Regression...\")\n",
    "mb.fitLogisticRegression(C=0.001, class_weight='balanced',max_iter=1000, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca55444",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31277fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.ConfusionMatricesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312547e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d745122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mb = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "# print(\"Prepareing Data...\")\n",
    "# _, _, _, _ = mb.prepareData(df, do_scale=True, do_sliding=False, do_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Fitting Decision Tree...\")\n",
    "# mb.fitDecisionTree(max_depth=None, min_samples_leaf=1, min_samples_split=2)\n",
    "# print(\"Fitting Random Forest...\")\n",
    "# mb.fitRandomForest(n_estimators=10, max_depth=None, min_samples_leaf=1, min_samples_split=2)\n",
    "# print(\"Fitting Logistic regression...\")\n",
    "# mb.fitLogisticRegression( C=1.0, solver=\"lbfgs\", max_iter=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4085860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mb.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mb.ConfusionMatricesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabf996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all but the excluded columns\n",
    "excluded = ['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality']\n",
    "columns = [c for c in df.columns if c not in excluded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44397b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.feature_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f0282ff",
   "metadata": {},
   "source": [
    "Welche Features sind besonders Aussagekräftig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7544ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = mb.random_forest.feature_importances_\n",
    "# forest = mb.random_forest\n",
    "# forest_importances = pd.Series(importances, index=mb.feature_columns)\n",
    "# std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "# fig, ax = plt.subplots()\n",
    "# forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "# ax.set_title(\"Feature importances using MDI\")\n",
    "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# fig.set_figheight(15)\n",
    "# fig.set_figwidth(30)\n",
    "# fig.tight_layout()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.plotFeatureImportances(feature_columns_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2b43d",
   "metadata": {},
   "source": [
    "Besonder Aussagekräftig sind Vorallem die Analogen Sensoren\n",
    "Vergleich training nur mit Analog Sensoren und mit allen Sensoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceeca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = ['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality']\n",
    "excluded.extend(digitial_sensors)\n",
    "#remove duplicates\n",
    "excluded = list(dict.fromkeys(excluded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metClf = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=900, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=excluded)\n",
    "print(\"Prepareing Data...\")\n",
    "_, _, _, _ = metClf.prepareData(df, do_scale=True, do_pca=False)\n",
    "\n",
    "#fit classifiers\n",
    "print(\"Fitting Decision Tree...\")\n",
    "metClf.fitDecisionTree(max_depth=None, min_samples_leaf=1, min_samples_split=2)\n",
    "print(\"Fitting Random Forest...\")\n",
    "metClf.fitRandomForest(n_estimators=10, max_depth=None, min_samples_leaf=1, min_samples_split=3)\n",
    "print(\"Fitting Logistic Regression...\")\n",
    "metClf.fitLogisticRegression(C=0.001, class_weight='balanced',max_iter=1000, solver='lbfgs')\n",
    "\n",
    "metClf.evalModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ff5b8",
   "metadata": {},
   "source": [
    "Ergebniss: Ohne Digitale Sensoren Modell minimal Besser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = metClf.random_forest.feature_importances_\n",
    "# forest = metClf.random_forest\n",
    "# forest_importances = pd.Series(importances, index=metClf.feature_columns)\n",
    "# std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "# fig, ax = plt.subplots()\n",
    "# forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "# ax.set_title(\"Feature importances using MDI\")\n",
    "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# fig.set_figheight(15)\n",
    "# fig.set_figwidth(30)\n",
    "# fig.tight_layout()\n",
    "# fig.show()\n",
    "\n",
    "# #print the 10 most importent features in descending order\n",
    "# forest_importances.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681da579",
   "metadata": {},
   "outputs": [],
   "source": [
    "metClf.plotFeatureImportances(feature_columns_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ecb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = metClf.mostImportantFeatures(index=10)\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ERKLÄRUNGEN HINZUFÜGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e3748",
   "metadata": {},
   "source": [
    "Werden nur Analoge Sensoren Verwendet, sind die Wichtigsten Sensoren:\n",
    "- **DV_pressure_mean**      [0.087352]\n",
    "- **DV_pressure_min**       [0.072631]\n",
    "- **Reservoirs_max**        [0.060411]\n",
    "- **Flowmeter_min**         [0.055333]\n",
    "- **Reservoirs_min**        [0.051933]\n",
    "- **TP3_std**               [0.050655]\n",
    "- **H1_std**                [0.049547]\n",
    "- **Oil_temperature_max**   [0.048480]\n",
    "- **Oil_temperature_mean**  [0.046990]\n",
    "- **Reservoirs_mean**       [0.046393]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b48998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c993957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #find best parameters for max_depth and min_samples_leaf on a range from 3 to 30 for best accuracy\n",
    "\n",
    "# best_accuracy = 0\n",
    "# best_params = (0, 0)\n",
    "\n",
    "# #column of the label\n",
    "# label = 'Label'\n",
    "# #only keep columns: TP3, LPS, DV_pressure, Oil_temperature\n",
    "# columns = ['TP3', 'LPS', 'DV_pressure', 'Oil_temperature']\n",
    "\n",
    "# for max_depth in range(3,15):\n",
    "#     for min_samples_leaf in range(3,12):\n",
    "#         clf = DecisionTreeBinaryClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, columns=columns)\n",
    "#         clf.fit(df=df, label=label)\n",
    "#         accuracy = clf.score()\n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_params = (max_depth, min_samples_leaf)\n",
    "\n",
    "# print(\"Best max_depth:\", best_params[0])\n",
    "# print(\"Best min_samples_leaf:\", best_params[1])\n",
    "# print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ea1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bc90c5b",
   "metadata": {},
   "source": [
    "### Aufgabe 2 (Vorhersage des Eintretens von Störungen)\n",
    "Erstellen Sie nun Klassifikationsmodelle, um anhand der gegebenen Sensormessdaten vorherzusagen, ob innerhalb eines bestimmten Zeitraums (z.B. 1 Stunde, 2 Stunden etc.) eine Storung der APU auftreten wird. Laut Betreiber ware es wünschenswert, mindestens zwei Stunden im Voraus eine Storung vorhersagen zu können, um rechtzeitig Maßnahmen einzuleiten, vgl. [1]. Testen Sie verschiedene Prognosezeitraume und stellen Sie die resultierenden Modelle gegen über."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e68f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEE:\n",
    "# Datensatz shiften um ein gewisses Window, labels aber gleich behalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e322307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_dataframe(df, hours):\n",
    "    \"\"\"\n",
    "    Shifts the dataframe by \"hours\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: The dataframe to shift\n",
    "    hours: The number of hours to shift the dataframe\n",
    "    \"\"\"\n",
    "    shiff_steps = 60 * 60 * hours\n",
    "    pd.options.display.max_columns = None\n",
    "\n",
    "    # Create a new dataframe, that has a extra column foreach column with the vlaue of the original column shifted by \"shiff_steps\" steps\n",
    "    df_shifted = pd.DataFrame()\n",
    "    for column in df.columns:\n",
    "        if column != 'Label' or column != 'timestamp':\n",
    "            df_shifted[column] = df[column].shift(shiff_steps)\n",
    "        if(column == 'Label' or column == 'timestamp'):\n",
    "            df_shifted[column] = df[column]\n",
    "    df_shifted.dropna(inplace=True)\n",
    "    return df_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf767e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shifted = shift_dataframe(df, 1)\n",
    "df_shifted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A single step is 1s \n",
    "# shiff_steps = 60*60*1 # 1a011\n",
    "# pd.options.display.max_columns = None\n",
    "\n",
    "# # Create a new dataframe, that has a extra column foreach column with the vlaue of the original column shifted by \"shiff_steps\" steps\n",
    "# df_shifted = pd.DataFrame()\n",
    "# for column in df.columns:\n",
    "#     if column != 'Label' or column != 'timestamp':\n",
    "#         df_shifted[column] = df[column].shift(shiff_steps)\n",
    "#         #df_shifted[column+\"(t)\"] = df[column]\n",
    "\n",
    "#     if(column == 'Label' or column == 'timestamp'):\n",
    "#         df_shifted[column] = df[column]\n",
    "\n",
    "# #drop nan rows\n",
    "# df_shifted.dropna(inplace=True)\n",
    "\n",
    "# df_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7babc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "#show the shifted_data around the second failure with a buffer of 3 hours before and after\n",
    "df_shifted[(df_shifted['timestamp'] > '2022-03-23 15:00:00') & (df_shifted['timestamp'] <= '2022-03-25 17:00:00')]\n",
    "#df[(df['timestamp'] > '2022-03-23 15:00:00') & (df['timestamp'] <= '2022-03-25 17:00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb94eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "futureBinaryClassifier = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "futureBinaryClassifier.prepareData(df_shifted, do_scale=False, do_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f490f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting Decision Tree...\")\n",
    "futureBinaryClassifier.fitDecisionTree(max_depth=None, min_samples_leaf=1, min_samples_split=2)\n",
    "print(\"Fitting Random Forest...\")\n",
    "futureBinaryClassifier.fitRandomForest(n_estimators=10, max_depth=None, min_samples_leaf=1, min_samples_split=2)\n",
    "print(\"Fitting Logistic regression...\")\n",
    "futureBinaryClassifier.fitLogisticRegression( C=1.0, solver=\"lbfgs\", max_iter=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(futureBinaryClassifier.randomForestScore())\n",
    "futureBinaryClassifier.evalModels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0cc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "futureBinaryClassifier.plotFeatureImportances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df52cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shifted_2 = shift_dataframe(df, 2)\n",
    "futureBinaryClassifier_2 = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "futureBinaryClassifier_2.prepareData(df_shifted_2, do_scale=False, do_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484f82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a399ad40",
   "metadata": {},
   "source": [
    "### Aufgabe 3 (Vorhersage der Dauer von Störungen)\n",
    "Entwickeln Sie Prognosemodelle zur Vorhersage der Storungsdauer und beurteilen Sie auf geeignete Weise deren Gute sowie deren Eignung für den Einsatz in der Praxis. Sofern diese aus Ihrer Sicht nicht ausreichend ist, skizzieren Sie Maßnahmen, durch die die Gute verbessert werden konnte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aus Tabelle\n",
    "err_1_start = dt.datetime(2022,2,28,21,53)\n",
    "err_1_end = dt.datetime(2022,3,1,2,00)\n",
    "err_2_start = dt.datetime(2022,3,23,14,54)\n",
    "err_2_end = dt.datetime(2022,3,23,15,24)\n",
    "err_3_start = dt.datetime(2022,5,30,12,00)\n",
    "err_3_end = dt.datetime(2022,6,2,6,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining = pd.read_csv('dataset_train.csv')\n",
    "df_remaining['timestamp'] = pd.to_datetime(df_remaining['timestamp'])\n",
    "\n",
    "# Zeiträume für Fehler 1-3 berechnen\n",
    "err_1_period = (err_1_start, err_1_end)\n",
    "err_2_period = (err_2_start, err_2_end)\n",
    "err_3_period = (err_3_start, err_3_end)\n",
    "\n",
    "# Zeiträume in einer Liste zusammenfassen\n",
    "failure_periods = [err_1_period, err_2_period, err_3_period]\n",
    "\n",
    "# Funktion zur Berechnung der verbleibenden Zeit für jedes Zeitintervall\n",
    "def get_remaining_time(timestamp, failure_periods):\n",
    "    for start, end in failure_periods:\n",
    "        if start <= timestamp <= end:\n",
    "            return (end - timestamp).total_seconds()\n",
    "    return 0\n",
    "\n",
    "# Anwenden der Funktion auf jede Zeile des DataFrames\n",
    "df_remaining['Label'] = df_remaining['timestamp'].apply(lambda x: get_remaining_time(x, failure_periods))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining[(df_remaining['timestamp'] > '2022-03-23 15:00:00') & (df_remaining['timestamp'] <= '2022-03-23 15:24:00')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the labels\n",
    "df_remaining['Label'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "c.prepareData(df_remaining, do_scale=False, do_pca=True,label_func = 'mean')\n",
    "# c.fitRandomForest()\n",
    "# print(c.randomForestScore())\n",
    "# c.randomForestConfusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0198f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELETE_ME = c.y_test.copy()\n",
    "\n",
    "#order by index\n",
    "DELETE_ME.sort_index(inplace=True)\n",
    "DELETE_ME.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0270ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.fitLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.linear_regression.predict(c.X_test)\n",
    "actual = c.y_test.copy()\n",
    "#reset the index of the actual values\n",
    "results = pd.DataFrame({'Actual': actual, 'Predicted': c.linear_regression.predict(c.X_test)})\n",
    "results.sort_index(inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by index\n",
    "results.sort_index(inplace=True)\n",
    "#plot the actual values and the predicted values\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40dccf1",
   "metadata": {},
   "source": [
    "* degree:  1  score:  0.016736459131068804\n",
    "* degree:  2  score:  0.10737802942146979\n",
    "* degree:  3  score:  0.15132775411259902\n",
    "* degree:  4  score:  0.2028557891968763\n",
    "* degree:  5  score:  0.2363742270730982\n",
    "* degree:  6  score:  0.2534498976049867\n",
    "* degree:  7  score:  0.2744239750665127\n",
    "* degree:  8  score:  0.28447377870745905\n",
    "* degree:  9  score:  0.2391308987311287\n",
    "* degree:  10  score:  -3.9171914332267095\n",
    "* degree:  11  score:  -6.665693831506913\n",
    "* degree:  12  score:  -50.20041370882595\n",
    "* degree:  13  score:  -292.0148401877538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f801b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #find best accuracy\n",
    "# for i in range(1, 20):\n",
    "#     c.fitPlynomialRegression(degree=i)\n",
    "#     print('degree: ', i, ' score: ', c.polyregScore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149e244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f166bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.fitPlynomialRegression(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f657f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.polyregScore()\n",
    "y_pred = c.polyreg_scaled.predict(c.X_test)\n",
    "\n",
    "actual = c.y_test.copy()\n",
    "#reset the index of the actual values\n",
    "actual.reset_index(drop=True, inplace=True)\n",
    "results = pd.DataFrame({'Actual': actual, 'Predicted': y_pred})\n",
    "\n",
    "plt.plot(actual, label='Actual')\n",
    "plt.plot(results['Predicted'], label='Predicted')\n",
    "plt.legend()\n",
    "#set the dimension of the plot so taht the x axis is not to small\n",
    "plt.rcParams[\"figure.figsize\"] = (40,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.polyregScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#order the results df by the actual values descending\n",
    "results.sort_values(by=['Actual'], ascending=False, inplace=True)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "plt.plot(actual, label='Actual')\n",
    "plt.plot(results['Predicted'], label='Predicted')\n",
    "plt.legend()\n",
    "#set the dimension of the plot so taht the x axis is not to small\n",
    "plt.rcParams[\"figure.figsize\"] = (40,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ee227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt with plotly\n",
    "import plotly.express as px\n",
    "fig = px.line(results, x=results.index, y=[\"Actual\", \"Predicted\"], title='Actual vs Predicted')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fae63e01",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. Ansatz: Verbleibende Zeit in Klassen aufteilen.\n",
    "Anstelle dass die Labels die verbleibende Zeit bis zum Ende der Störung sind, werden die Labels die verbleibende Zeit in Klassen aufgeteilt.\n",
    "Die Klassen sind x verbleibende Minuten. So kann ein Klassifikator auf das Problem angewand werden, alledings kann bei diesem Ansatz selbst ein perfekter klassifikator die zeit nur so genau vorhersagen wie die Klassen aufgeteilt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3aeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(df, err_starts, err_ends, x_minutes):\n",
    "    \"\"\"\n",
    "    Generates Labels in a dataframe. The label is the remaining time until the end of the current failure. if there is no failure, the label is 0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to generate the labels for\n",
    "    err_starts : list\n",
    "        A list of datetime objects, which are the start times of the failures\n",
    "    err_ends : list\n",
    "        A list of datetime objects, which are the end times of the failures\n",
    "    x_minutes : int\n",
    "        The number of minutes to divide the remaining time into\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['Label'] = 0\n",
    "    \n",
    "    for i, (err_start, err_end) in enumerate(zip(err_starts, err_ends)):\n",
    "        condition = (df['timestamp'] > err_start) & (df['timestamp'] <= err_end)\n",
    "        df.loc[condition, 'Label'] = np.ceil((err_end - df.loc[condition, 'timestamp']).dt.total_seconds() / (60 * x_minutes))\n",
    "    return df\n",
    "\n",
    "X_MINUTES = 10\n",
    "df_remaining = pd.read_csv('dataset_train.csv')\n",
    "df_remaining = generate_labels(df_remaining, [err_1_start, err_2_start, err_3_start], [err_1_end, err_2_end, err_3_end], X_MINUTES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining[(df_remaining['timestamp'] > '2022-03-23 15:00:00') & (df_remaining['timestamp'] <= '2022-03-23 15:24:00')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cef20b41",
   "metadata": {},
   "source": [
    "Jetzt können wieder die gleichen Klassifikatoren wie oben angewandt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier = MetroBinaryClassifier(use_shortened=True, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "_,_,_,_ = remaining_classifier.prepareData(df_remaining, do_scale=True, do_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting Decision Tree...\")\n",
    "remaining_classifier.fitDecisionTree(max_depth=None, min_samples_leaf=1, min_samples_split=2)\n",
    "print(\"Fitting Random Forest...\")\n",
    "remaining_classifier.fitRandomForest(n_estimators=10, max_depth=None, min_samples_leaf=1, min_samples_split=2)\n",
    "print(\"Fitting Logistic regression...\")\n",
    "remaining_classifier.fitLogisticRegression( C=1.0, solver=\"lbfgs\", max_iter=1000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21968be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining_classifier.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7eb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining_classifier.getPredictScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier.plotSingleConfusionMatrix(remaining_classifier.random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier.plotSingleActualPredicted(remaining_classifier.random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5895df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining_classifier.ConfusionMatricesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining_classifier.randomForestConfusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(remaining_classifier.y_test, remaining_classifier.random_predict)\n",
    "# cm\n",
    "# cm.shape\n",
    "# #remove the 0 columns from the confusion matrix(because the majority of the data is 0)\n",
    "# cm = cm[:,1:]\n",
    "# #plot the confusion matrix\n",
    "# plt.figure(figsize=(20,14))\n",
    "# sns.heatmap(cm, annot=False)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Truth')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res = pd.DataFrame({'Actual': remaining_classifier.y_test, 'Predicted': remaining_classifier.random})\n",
    "# #sort by index\n",
    "# df_res.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "# #plot with plotly\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Actual'], name='Actual'))\n",
    "# fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Predicted'], name='Predicted'))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df15ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a783a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remaining_classifier.randomForestConfusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e77931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remaining_classifier.ConfusionMatricesis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f224287d",
   "metadata": {},
   "source": [
    "### Aufgabe 4 (Vorhersage der gestorten Komponente) \n",
    "Untersuchen Sie, ob sich der Datensatz auch dazu eignet, die von einer Storung betroffenen Komponente anhand der Sensordaten zu identifizieren. Erstellen und evaluieren Sie dazu entsprechende Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from csv, because we need to reassing the labels\n",
    "df_type = pd.read_csv('dataset_train.csv')\n",
    "df_type['timestamp'] = pd.to_datetime(df_type['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the 3 error timespans set a label colum that from 1 to 3 and the rest to 0\n",
    "#Aus Tabelle\n",
    "err_1_start = dt.datetime(2022,2,28,21,53)\n",
    "err_1_end = dt.datetime(2022,3,1,2,00)\n",
    "err_2_start = dt.datetime(2022,3,23,14,54)\n",
    "err_2_end = dt.datetime(2022,3,23,15,24)\n",
    "err_3_start = dt.datetime(2022,5,30,12,00)\n",
    "err_3_end = dt.datetime(2022,6,2,6,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bc752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#add a label column to the dataframe that is only zeros\n",
    "df_type['Label'] = 0\n",
    "\n",
    "#if the data is between err_1_start and err_1_end set the label to 1\n",
    "df_type.loc[(df_type['timestamp'] >= err_1_start) & (df_type['timestamp'] <= err_1_end), 'Label'] = 1\n",
    "\n",
    "#if the data is between err_2_start and err_2_end set the label to 2\n",
    "df_type.loc[(df_type['timestamp'] >= err_2_start) & (df_type['timestamp'] <= err_2_end), 'Label'] = 2\n",
    "\n",
    "#if the data is between err_3_start and err_3_end set the label to 3\n",
    "df_type.loc[(df_type['timestamp'] >= err_3_start) & (df_type['timestamp'] <= err_3_end), 'Label'] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=30, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "_,_,_,_ = clf.prepareData(df_type, do_scale=False, do_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fitRandomForest()\n",
    "clf.fitDecisionTree()\n",
    "clf.fitNaiveBayes()\n",
    "clf.fitLinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7b95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.randomForestConfusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf215bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef6ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba25fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.ConfusionMatricesis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4555b60f",
   "metadata": {},
   "source": [
    "### Aufgabe 5 (Störungserkennung mit Hilfe von Unsupervised Learning)\n",
    "Eine Herausforderung bei der Modellbildung fur Predictive Maintenance ist häufig das Fehlen von Informationen zu historischen Storungen, sodass Ansätze des Supervised Learning nicht anwendbar sind. In diesem Fall konnen Methoden des Unsupervised Learning eine Option sein. Wenden Sie auf den Datensatz aus Aufgabe 1 (ohne Labels) ein Clustering-Verfahren an und\n",
    "uberprüfen Sie anhand der gegebenen Informationen zu den historischen Systemausfällen, ob und wie gut sich durch einen solchen Ansatz Ausnahmezustande (Anomalien bzw. Störungen) von \"normalen\" Systemzustanden der APU unterscheiden lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) | \n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5273be",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "df_sliding = p.sliding_window(df, 60)\n",
    "\n",
    "y_true = df_sliding['Label']\n",
    "df_sliding.drop(['Label'], axis=1, inplace=True)\n",
    "\n",
    "df_sliding = StandardScaler().fit_transform(df_sliding) # normalize data\n",
    "\n",
    "#df_sliding, _ = p.performPca(df_sliding, df_sliding)\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# number of clusters\n",
    "k = 2\n",
    "\n",
    "# create and fit k-means model\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(df_sliding)\n",
    "\n",
    "# predict cluster labels for new data\n",
    "labels = kmeans.predict(df_sliding)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique values of the labels array\n",
    "\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5476d8b",
   "metadata": {},
   "source": [
    "Beste Clustergröße herausfinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14367ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(df_sliding)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(1, 11), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552398eb",
   "metadata": {},
   "source": [
    "WCSS\n",
    "* 1: 2097494814.352996\n",
    "* 2: 1222016429.5818548\n",
    "* 3: 697675985.9232541\n",
    "* 4: 561449869.0233672\n",
    "* 5: 459598615.93620276\n",
    "* 6: 370670666.4551599\n",
    "* 7: 326371468.84605604\n",
    "* 8: 292667190.21452206\n",
    "* 9: 262847830.99261418\n",
    "* 10: 240391251.9439149\n",
    "\n",
    "Gwählt 6, weil ab da die WCSS nicht mehr so stark sinkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train auf cluster 6\n",
    "\n",
    "# # create and fit k-means model\n",
    "# kmeans = KMeans(n_clusters=8)\n",
    "# kmeans.fit(df_sliding)\n",
    "\n",
    "# # predict cluster labels for new data\n",
    "# labels = kmeans.predict(df_sliding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe out of df_sliding\n",
    "df_sliding = pd.DataFrame(df_sliding)\n",
    "df_sliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba529b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_labels = df_sliding.copy()\n",
    "df_both_labels['Label'] = y_true\n",
    "df_both_labels['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88987cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the label and cluster columns in a line chart\n",
    "df_both_labels.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_labels['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "#show the part around error 1\n",
    "\n",
    "df_both_labels.loc[(df_both_labels['timestamp'] >= err_1_start - dt.timedelta(0,600)) & (df_both_labels['timestamp'] <= err_1_end + dt.timedelta(0,16000)), 'Label'].plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98586873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_labels['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "around_err_1 = df_both_labels.loc[(df_both_labels['timestamp'] >= err_3_start- dt.timedelta(0,600)) & (df_both_labels['timestamp'] <= err_3_end + dt.timedelta(0,16000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "around_err_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b46796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the labels around err1\n",
    "\n",
    "around_err_1.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliding_with_both_lables = df_sliding.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be3fcc",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd081ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) | \n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "df_sliding = p.sliding_window(df, 60)\n",
    "y_true = df['Label']\n",
    "df_sliding.drop(['Label'], axis=1, inplace=True)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "df_sliding = pca.fit_transform(df_sliding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAuerT SEHR LANGE!\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize the DBSCAN model\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "\n",
    "# Fit the model to your data\n",
    "dbscan.fit(df_sliding)\n",
    "\n",
    "# Obtain the cluster labels for each data point\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37abb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620df00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2f6a2ec",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "Ansatz GaussianMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) | \n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aefa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliding = p.sliding_window(df, 60)\n",
    "y_true = df_sliding['Label']\n",
    "\n",
    "#scale data\n",
    "\n",
    "df_sliding.drop(['Label'], axis=1, inplace=True)\n",
    "df_sliding = StandardScaler().fit_transform(df_sliding) # normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d397c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create an instance of the GMM class\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "\n",
    "# Fit the model to your data\n",
    "gmm.fit(df_sliding)\n",
    "\n",
    "# Predict the cluster assignments for each data point\n",
    "cluster_labels = gmm.predict(df_sliding)\n",
    "\n",
    "# Get the cluster means\n",
    "cluster_means = gmm.means_\n",
    "\n",
    "# Get the cluster covariances\n",
    "cluster_covariances = gmm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d2193",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782443bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the labels and the cluster labels\n",
    "labels_df = pd.DataFrame({'Label': y_true, 'Cluster': cluster_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607691cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the parts where label is 1\n",
    "labels_df.loc[labels_df['Label'] == 1].plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e193cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.plot.scatter(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define threshold\n",
    "threshold = 3\n",
    "\n",
    "# Define function to apply to rolling window\n",
    "def check_consecutive(window):\n",
    "    current_value = window.iloc[0]\n",
    "    consecutive_count = sum(window == current_value)\n",
    "    if consecutive_count >= threshold:\n",
    "        return current_value\n",
    "    else:\n",
    "        return window.iloc[-1]  \n",
    "\n",
    "new = pd.DataFrame()\n",
    "# Apply function to rolling window\n",
    "new['smooth_cluster'] = labels_df['Cluster'].rolling(threshold).apply(check_consecutive, raw=False)\n",
    "\n",
    "# Forward fill to propagate the last valid value\n",
    "new['smooth_cluster'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "new['Lable'] = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31225bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a plotly plot from the same data as above\n",
    "fig = px.line(labels_df.loc[labels_df['Label'] == 1], y=['Cluster', 'Label'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115b1e5",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "#### Noch probieren:\n",
    "* K-Means mit rohdaten(kein rolling window)\n",
    "* Probieren 2 Sensorwerte plotten für kmeans\n",
    "* Probieren nur wichtige collumns für kmeans (kein PCA sondern reines ausschneiden der werte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datensatz einlsesn \n",
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "#Label erstellen\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) |\n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a50f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betrachtet werden hier jetzt mal nur TP3 und Oil_Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42881852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index of the row that has df['timestamp] == err_1_start\n",
    "err_1_start_index = df.index[df['timestamp'] == err_1_start].tolist()[0]\n",
    "err_1_end_index = df.index[df['timestamp'] == err_1_end].tolist()[0]\n",
    "err_2_start_index = df.index[df['timestamp'] == err_2_start].tolist()[0]\n",
    "err_2_end_index = df.index[df['timestamp'] == err_2_end].tolist()[0]\n",
    "err_3_start_index = df.index[df['timestamp'] == err_3_start].tolist()[0]\n",
    "err_3_end_index = df.index[df['timestamp'] == err_3_end].tolist()[0]\n",
    "#print both indexes\n",
    "print(\"Start Idx: \", err_1_start_index, \"| End Idx: \", err_1_end_index)\n",
    "\n",
    "#add 10 minutes to err1 start index\n",
    "err_1_start_index_t = err_1_start_index - 60000 # 10 minutes (1 datapoint per second)\n",
    "\n",
    "print(\"Start Idx: \", err_1_start_index_t, \"| End Idx: \", err_1_end_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot label between err_1_start_index_t and err_1_end_index\n",
    "df.loc[err_1_start_index_t:err_1_end_index, 'Label'].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the data between err_1_start_index_t and err_1_end_index\n",
    "df.loc[err_1_start_index_t:err_1_end_index].plot.line(x='timestamp', y=['TP3', 'Oil_temperature', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the oil temperature against TP3 between err_1_start_index_t and err_1_end_index in a scatter plot\n",
    "df.loc[err_1_start_index_t:err_1_end_index].plot.scatter(x='TP3', y='Oil_temperature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e505202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature']]\n",
    "\n",
    "#scale data\n",
    "df_tmp = pd.DataFrame(StandardScaler().fit_transform(df_tmp),columns=df_tmp.columns) # normalize data\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc0f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df774934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the weights of the clusters\n",
    "kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686eacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the oil temperature against TP3 between err_1_start_index_t and err_1_end_index in a scatter plot and include the cluster centers\n",
    "df_tmp.plot.scatter(x='TP3', y='Oil_temperature')\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='red', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c732f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = kmeans.predict(df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.plot.scatter(x='TP3', y='Oil_temperature', c='Cluster', colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame()\n",
    "df_labels['Label'] = df.loc[err_1_start_index_t:err_1_end_index, 'Label']\n",
    "df_labels['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we try it with 3 sensors\n",
    "\n",
    "df_3d = df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature', 'Reservoirs']].copy()\t\n",
    "\n",
    "#scale data\n",
    "df_3d = pd.DataFrame(StandardScaler().fit_transform(df_3d),columns=df_3d.columns) # normalize data\n",
    "\n",
    "#perform kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(df_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d plot of the data\n",
    "fig = px.scatter_3d(df_3d, x='TP3', y='Oil_temperature', z='Reservoirs', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the labeling\n",
    "df_labels = pd.DataFrame()\n",
    "df_labels['Label'] = df.loc[err_1_start_index_t:err_1_end_index, 'Label']\n",
    "df_labels['Cluster'] = kmeans.labels_\n",
    "\n",
    "df_labels.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7629b3",
   "metadata": {},
   "source": [
    "--------------\n",
    "KMeans nur mit den wichtigsten collumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove label from the excluded list\n",
    "excluded.remove('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all excluded columns\n",
    "df_essentials = df.drop(columns=excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit kemans in the test piod between err_1_start_index_t and err_1_end_index\n",
    "df_essentails_test = df_essentials.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "#scale the data\n",
    "l = df_essentails_test['Label']\n",
    "df_essentails_test = df_essentails_test.drop(columns=['Label'], axis=1)\n",
    "df_essentails_test = pd.DataFrame(MinMaxScaler().fit_transform(df_essentails_test),columns=df_essentails_test.columns) # normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(df_essentails_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23068da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e677518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essentails_test['Cluster'] = kmeans.labels_\n",
    "l = l.reset_index()\n",
    "df_essentails_test['Label'] = l['Label']\n",
    "df_essentails_test.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df31866",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essentials = df.drop(columns=excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOverlap(y_true_s, cluster):\n",
    "    #this function takes 2 arrays with labels and calculates how many labels are the same.\n",
    "    #the result is a percentage of how many labels are the same\n",
    "    #the higher the percentage the better the clustering\n",
    "    #every position in one array is checked against the other array\n",
    "    #after that the labels of the clustering are switched and the process is repeated, because we do not know which label is which\n",
    "    #the higher percentage is returned\n",
    "\n",
    "    #calculate the percentage of the first try\n",
    "    y_true = y_true_s.values\n",
    "\n",
    "    print(len(y_true), len(cluster))\n",
    "    percentage = 0\n",
    "    for i in range(len(y_true)-1):\n",
    "        if y_true[i] == cluster[i]:\n",
    "            percentage += 1\n",
    "    percentage = percentage / len(y_true)\n",
    "\n",
    "    #calculate the percentage of the second try\n",
    "    percentage2 = 0\n",
    "    for i in range(len(y_true)-1):\n",
    "        if y_true[i] != cluster[i]:\n",
    "            percentage2 += 1\n",
    "    percentage2 = percentage2 / len(y_true)\n",
    "\n",
    "    #return the higher percentage\n",
    "    if percentage > percentage2:\n",
    "        return percentage\n",
    "    else:\n",
    "        return percentage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e60493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a loop where we repeat the process, but we alwas remove one colum from the dataframe, and comapre the results\n",
    "\n",
    "accuracys = []\n",
    "for column in df_essentials.columns:\n",
    "    if(column == 'Label'):\n",
    "        continue\n",
    "    print(\"Without: \", column)\n",
    "    copy = df_essentials.copy()\n",
    "    copy = copy.drop(columns=column, axis=1,inplace=False )\n",
    "    print(\"columns: \", copy.columns)\n",
    "    df_essentails_test = copy.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "    y = df_essentails_test['Label']\n",
    "    df_essentails_test.drop(columns='Label', axis=1,inplace=True )\n",
    "\n",
    "    #scale the data\n",
    "    df_essentails_test = pd.DataFrame(MinMaxScaler().fit_transform(df_essentails_test),columns=df_essentails_test.columns) # normalize data\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(df_essentails_test)\n",
    "    df_essentails_test['Cluster'] = kmeans.labels_\n",
    "    df_essentails_test['Label'] = y\n",
    "    #df_essentails_test.plot.line(y=['Cluster', 'Label'])\n",
    "    print(checkOverlap(y, kmeans.labels_))\n",
    "    accuracys.append((checkOverlap(y, kmeans.labels_), column))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print orderd accuracys\n",
    "accuracys.sort(reverse=True)\n",
    "accuracys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same thing as the loop above, but now we remove 2 columns\n",
    "accuracys2 = []\n",
    "for column in df_essentials.columns:\n",
    "    if(column == 'Label'):\n",
    "        continue\n",
    "    for column2 in df_essentials.columns:\n",
    "        if(column2 == 'Label' or column2 == column):\n",
    "            continue\n",
    "        print(\"Without: \", column, column2)\n",
    "        copy = df_essentials.copy()\n",
    "        copy = copy.drop(columns=[column, column2], axis=1,inplace=False )\n",
    "        print(\"columns: \", copy.columns)\n",
    "        df_essentails_test = copy.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "        y = df_essentails_test['Label']\n",
    "        df_essentails_test.drop(columns='Label', axis=1,inplace=True )\n",
    "        #scale the data\n",
    "        df_essentails_test = pd.DataFrame(MinMaxScaler().fit_transform(df_essentails_test),columns=df_essentails_test.columns) # normalize data\n",
    "        kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(df_essentails_test)\n",
    "        df_essentails_test['Cluster'] = kmeans.labels_\n",
    "        df_essentails_test['Label'] = y\n",
    "        #df_essentails_test.plot.line(y=['Cluster', 'Label'])\n",
    "        print(checkOverlap(y, kmeans.labels_))\n",
    "        accuracys2.append((checkOverlap(y, kmeans.labels_), column, column2))\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys2.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30180166",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try cmeans with te best 2 columns\n",
    "copy = df_essentials.copy()\n",
    "copy = copy.drop(columns=['TP2', 'Flowmeter'], axis=1,inplace=False)\n",
    "print(\"columns: \", copy.columns)\n",
    "df_essentails_test = copy.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "y = df_essentails_test['Label']\n",
    "df_essentails_test.drop(columns='Label', axis=1,inplace=True )\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(df_essentails_test)\n",
    "df_essentails_test['Cluster'] = kmeans.labels_\n",
    "df_essentails_test['Label'] = y\n",
    "df_essentails_test.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no try kmeans on the entire dataset\n",
    "\n",
    "copy = df_essentials.copy()\n",
    "\n",
    "copy = copy.drop(columns=['TP2', 'Flowmeter'], axis=1,inplace=False)\n",
    "print(\"columns: \", copy.columns)\n",
    "df_essentails_test = copy.copy()\n",
    "y = df_essentails_test['Label']\n",
    "df_essentails_test.drop(columns='Label', axis=1,inplace=True )\n",
    "#scale the data\n",
    "df_essentails_test = pd.DataFrame(MinMaxScaler().fit_transform(df_essentails_test),columns=df_essentails_test.columns) # normalize data\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(df_essentails_test)\n",
    "df_essentails_test['Cluster'] = kmeans.labels_\n",
    "df_essentails_test['Label'] = y\n",
    "df_essentails_test.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded.remove('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform pca to reduce the dimensions to 3 dimensions, to make i tpossible to plot the data\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "#excluded.remove('Label')\n",
    "df_essentials = df.drop(columns=excluded)\n",
    "y = df_essentials['Label']\n",
    "df_essentials.drop(columns=['Label'], axis=1,inplace=True )\n",
    "#scale the data\n",
    "df_essentials = pd.DataFrame(MinMaxScaler().fit_transform(df_essentials),columns=df_essentials.columns) # normalize data\n",
    "\n",
    "principalComponents = pca.fit_transform(df_essentials)\n",
    "pc = pd.DataFrame(principalComponents, columns=['PC1', 'PC2', 'PC3'])\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21782049",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_test = pc.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "pc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(pc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d scatterplot of the data\n",
    "fig = px.scatter_3d(pc_test, x='PC1', y='PC2', z='PC3', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = y.loc[err_1_start_index_t:err_1_end_index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pc_test, x='PC1', y='PC2', z='PC3', color=labs)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19622f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pc_test.index, y=labs,\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "fig.add_trace(go.Scatter(x=pc_test.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef0f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1817e67",
   "metadata": {},
   "source": [
    "-------\n",
    "try with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7abd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(data: DataFrame, window_size = 60):\n",
    "        print(\"SL\")\n",
    "        columns = data.columns\n",
    "        operations = ['mean', 'std', 'min', 'max']\n",
    "\n",
    "        #generate a dict, where the keys are the column names and the values are the operations that should be performed on the column\n",
    "        operations_dict = {}\n",
    "        for column in columns:\n",
    "            if(column == 'Label'):\n",
    "                continue\n",
    "            operations_dict[column] = operations\n",
    "\n",
    "        print(operations_dict)\n",
    "        df_rolling = data.rolling(window_size).agg(operations_dict)\n",
    "\n",
    "        #drop all columns that have NaN values\n",
    "        df_rolling = df_rolling.dropna()\n",
    "\n",
    "        #flatten df_rolling\n",
    "        df_rolling.columns = ['_'.join(col) for col in df_rolling.columns]\n",
    "\n",
    "        #keep original LabelValues\n",
    "        df_rolling['Label'] = data['Label']\n",
    "\n",
    "        #rename Label_min to Label\n",
    "        #df_rolling = df_rolling.rename(columns={'Label_max': 'Label'})\n",
    "        return df_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df.drop(columns=excluded)\n",
    "\n",
    "#try with only TP3, oiltemp and \n",
    "\n",
    "df_s = df_s.loc[err_1_start_index_t:err_1_end_index].copy()\n",
    "\n",
    "df_ss= sliding_window(df_s, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b448da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "df_ss = pd.DataFrame(MinMaxScaler().fit_transform(df_ss),columns=df_ss.columns) # normalize data\n",
    "\n",
    "#perform pca to reduce the dimensions to 3 dimensions, to make i tpossible to plot the data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "y = df_ss['Label']\n",
    "df_ss.drop(columns=['Label'], axis=1,inplace=True )\n",
    "principalComponents = pca.fit_transform(df_ss)\n",
    "pc = pd.DataFrame(principalComponents, columns=['PC1', 'PC2', 'PC3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ad8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d scatter plot with actual labels\n",
    "fig = px.scatter_3d(pc, x='PC1', y='PC2', z='PC3', color=y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform kmeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(pc)\n",
    "#3d scatterplot of the data\n",
    "fig = px.scatter_3d(pc, x='PC1', y='PC2', z='PC3', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187201b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------- see what happens without pca ----------------------------\n",
    "\n",
    "#perform kmeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(df_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_ss.index, y=y,\n",
    "                    mode='lines',\n",
    "                    name='y'))\n",
    "fig.add_trace(go.Scatter(x=df_ss.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='clusters'))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- try with only oil_temp TP3 and reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68732c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "analog_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b36dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['TP3', 'Reservoirs', 'Oil_temperature', 'Label']\n",
    "\n",
    "#generate a df where only the columns in keep are present\n",
    "df_small = df[keep]\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf557de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d plot the data\n",
    "df_small = df_small.loc[err_1_start_index_t:err_1_end_index].copy()\n",
    "fig = px.scatter_3d(df_small, x='TP3', y='Reservoirs', z='Oil_temperature', color='Label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "#import scaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_small['Label']\n",
    "df_small.drop(columns=['Label'], axis=1,inplace=True )\n",
    "\n",
    "#scale the data\n",
    "df_small = pd.DataFrame(MinMaxScaler().fit_transform(df_small),columns=df_small.columns) # normalize data\n",
    "\n",
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(df_small)\n",
    "\n",
    "#scatter 3d\n",
    "fig = px.scatter_3d(df_small, x='TP3', y='Reservoirs', z='Oil_temperature', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4aba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_small.index, y=y,\n",
    "                    mode='lines',\n",
    "                    name='y'))\n",
    "fig.add_trace(go.Scatter(x=df_small.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='clusters'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try on the entire dataset\n",
    "df_small = df[keep]\n",
    "\n",
    "df_small = sliding_window(df_small, 60)\n",
    "\n",
    "#scale the data\n",
    "y = df_small['Label']\n",
    "df_small.drop(columns=['Label'], axis=1,inplace=True )\n",
    "\n",
    "\n",
    "df_small = pd.DataFrame(MinMaxScaler().fit_transform(df_small),columns=df_small.columns) # normalize data\n",
    "\n",
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(df_small)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot real and predicted labels in matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y, label='y')\n",
    "ax.plot(kmeans.labels_, label='clusters')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = pd.DataFrame({'y':y, 'clusters':kmeans.labels_})\n",
    "df_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a90a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show df_l around error 1\n",
    "df_l.loc[err_2_start_index-600:err_2_end_index+2000].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da869867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create an instance of the GMM class\n",
    "gmm = GaussianMixture(n_components=4)\n",
    "\n",
    "# Fit the model to your data\n",
    "gmm.fit(df_small)\n",
    "\n",
    "# Predict the cluster assignments for each data point\n",
    "cluster_labels = gmm.predict(df_small)\n",
    "\n",
    "# Get the cluster means\n",
    "cluster_means = gmm.means_\n",
    "\n",
    "# Get the cluster covariances\n",
    "cluster_covariances = gmm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb97420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = pd.DataFrame({'y':y, 'clusters':cluster_labels})\n",
    "df_l.loc[err_1_start_index_t:err_1_end_index].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e312b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d scatter plot around error 3\n",
    "fig = px.scatter_3d(df_small.loc[err_3_start_index-600:err_3_end_index+2000], x='TP3', y='Reservoirs', z='Oil_temperature', color=cluster_labels[err_3_start_index-600:err_3_end_index+2000+1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa823a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(y, label='y')\n",
    "ax.plot(cluster_labels, label='clusters')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dbb1a",
   "metadata": {},
   "source": [
    "* Reservoirs_mean        0.123420\n",
    "* Reservoirs_min         0.062413\n",
    "* DV_pressure_mean       0.060066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842db2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drölf = df.copy()\n",
    "test_drölf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e188a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drölf = df.copy()\n",
    "#excluded.remove('Label')\n",
    "#remove exluded columns\n",
    "test_drölf.drop(columns=excluded, axis=1,inplace=True )\n",
    "\n",
    "test_drölf = sliding_window(test_drölf, 60)\n",
    "\n",
    "keep = ['Reservoirs_mean', 'Reservoirs_min', 'DV_pressure_mean', 'Label']\n",
    "\n",
    "test_drölf = test_drölf[keep]\n",
    "\n",
    "test_drölf = pd.DataFrame(MinMaxScaler().fit_transform(test_drölf),columns=test_drölf.columns) # normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d scatter plot around error 1\n",
    "s = test_drölf.loc[err_1_start_index-600:err_1_end_index+2000]\n",
    "fig = px.scatter_3d(s, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=s.index, size=s['Label']+1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test_drölf['Label']\n",
    "y_s = s['Label']\n",
    "s.drop(columns=['Label'], axis=1,inplace=True)\n",
    "test_drölf.drop(columns=['Label'], axis=1,inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(s)\n",
    "\n",
    "#scatter 3d\n",
    "s = test_drölf.loc[err_1_start_index-600:err_1_end_index+2000]\n",
    "fig = px.scatter_3d(s, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4584f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "y_cut = y[err_1_start_index]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=s.index, y=y_s,\n",
    "                    mode='lines',\n",
    "                    name='y'))\n",
    "fig.add_trace(go.Scatter(x=s.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='clusters'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do it on the entire dataset\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(test_drölf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'y':y, 'clusters':kmeans.labels_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d324e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = res.loc[err_1_start_index-600:err_1_end_index+2000]\n",
    "res_2 = res.loc[err_2_start_index-600:err_2_end_index+2000]\n",
    "res_3 = res.loc[err_3_start_index-600:err_3_end_index+2000]\n",
    "\n",
    "slice1 = test_drölf.loc[err_1_start_index-600:err_1_end_index+2000]\n",
    "slice2 = test_drölf.loc[err_2_start_index-600:err_2_end_index+2000]\n",
    "slice3 = test_drölf.loc[err_3_start_index-600:err_3_end_index+2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_3.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ba17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #3d scatter\n",
    "# fig = px.scatter_3d(slice1, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=res_1['clusters'])\n",
    "# fig.show()\n",
    "# fig = px.scatter_3d(slice1, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=res_1['y'])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7942032",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.concat([slice1, slice2, slice3])\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 3 slices together, \n",
    "cl = pd.concat([res_1, res_2, res_3])\n",
    "\n",
    "#scatter plot\n",
    "fig = px.scatter_3d(cat, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=cl['clusters'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869a51c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ca2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c074e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d71c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb91db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d451ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded.remove('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eab252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll = df.drop(columns=excluded, axis=1,inplace=False)\n",
    "y_ll = df_ll['Label']\n",
    "\n",
    "df_ll.drop(columns=['Label'], axis=1,inplace=True)\n",
    "\n",
    "df_ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale\n",
    "df_ll_scale = pd.DataFrame(MinMaxScaler().fit_transform(df_ll),columns=df_ll.columns) # normalize data\n",
    "df_ll_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll_short = df_ll_scale.loc[err_1_start_index-20000:err_1_end_index-15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the oil temperature\n",
    "df_ll_short['Oil_temperature'].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b01a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfc790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('MachineLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc9b5ad24f7c64b5a327ba84ead6ad3fbc2ebc7246c1e6195a5d7b169b73b46a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
