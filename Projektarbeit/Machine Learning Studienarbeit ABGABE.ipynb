{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87dd0cd",
   "metadata": {},
   "source": [
    "# Studienarbeit: Machine Learning\n",
    "#### Saniye Ogul und Johannes Horst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6569b20e",
   "metadata": {},
   "source": [
    "### Imports\n",
    "- plotly\n",
    "- matplotlib\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "#%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#import graphviz\n",
    "from sklearn import tree\n",
    "from matplotlib.dates import DateFormatter\n",
    "import datetime as dt\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7104e",
   "metadata": {},
   "source": [
    "#### Analyse des Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab10252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_time(s, fmat=\"%d-%m-%y %H:%M\"): return dt.datetime.strptime(s, fmat)\n",
    "\n",
    "def get_err(df, e, td_before=None, td_after=None): \n",
    "    dt_start = e[0]\n",
    "    if td_before: dt_start = dt_start - td_before\n",
    "        \n",
    "    dt_end = e[1]\n",
    "    if td_after: dt_end = dt_end + td_after\n",
    "        \n",
    "    return df.loc[(df[\"timestamp\"] >= dt_start) & (df[\"timestamp\"] < dt_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERRORS = [\n",
    "#    Start,             Ende,              Fehlernummer\n",
    "    (to_time(\"28-02-22 21:53\"), to_time(\"01-03-22 02:00\"), 1),\n",
    "    (to_time(\"23-03-22 14:54\"), to_time(\"23-03-22 15:24\"), 2),\n",
    "    (to_time(\"30-05-22 12:00\"), to_time(\"02-06-22 06:18\"), 3),\n",
    "]\n",
    "\n",
    "E1 = ERRORS[0]\n",
    "E2 = ERRORS[1]\n",
    "E3 = ERRORS[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_withLables():\n",
    "    df                 = pd.read_csv('dataset_train.csv')\n",
    "    df['timestamp']    = pd.to_datetime(df['timestamp'])\n",
    "    #df.set_index(\"timestamp\", verify_integrity=True, inplace=True)\n",
    "\n",
    "    exclude            = [\"gpsLong\", \"gpsLat\", \"gpsSpeed\", \"gpsQuality\"]\n",
    "    df                 = df[[c for c in df.columns if c not in exclude]]\n",
    "\n",
    "    df[\"Error\"]        = 0\n",
    "    for e_start, e_end, e_nr in ERRORS: df.loc[(df[\"timestamp\"] >= e_start) & (df[\"timestamp\"] < e_end), \"Error\"] = e_nr\n",
    "    df[\"isError\"]      = df[\"Error\"] != 0\n",
    "    return df\n",
    "\n",
    "df = read_data_withLables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558aab6",
   "metadata": {},
   "source": [
    "##### Visualisierung der Betriebszeiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ad00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"start_stop\"]   = 0\n",
    "df.loc[df[\"timestamp\"].diff(1).dt.total_seconds() != 1, \"start_stop\"] = 1\n",
    "df.loc[df[df[\"start_stop\"]==1].index[1:] - 1, \"start_stop\"]           = 2\n",
    "df.loc[df.iloc[-1].name, \"start_stop\"]                                = 2\n",
    "\n",
    "\n",
    "df_start            = df[df[\"start_stop\"] == 1].reset_index().rename(columns={\"timestamp\":\"start\"})\n",
    "df_stop             = df[df[\"start_stop\"] == 2].reset_index().rename(columns={\"timestamp\":\"stop\"})\n",
    "\n",
    "df_sp               = pd.concat([df_start[\"start\"], df_stop[\"stop\"]], axis=1)\n",
    "df_sp[\"duration\"]   = df_sp[\"stop\"] - df_sp[\"start\"]\n",
    "df_series           = df_sp.copy()\n",
    "df_series[\"values\"] = df_sp[\"duration\"].dt.total_seconds().astype(int)\n",
    "\n",
    "df_sp[\"Error\"]      = 0\n",
    "df_err = pd.DataFrame(ERRORS, columns=(\"start\", \"stop\", \"Error\"))\n",
    "df_err[\"duration\"] = df_err[\"stop\"] - df_err[\"start\"]\n",
    "df_sp  = pd.concat([df_sp, df_err]).reset_index(drop=True)\n",
    "\n",
    "df_sp[\"start_time\"] = pd.to_datetime(df_sp[\"start\"].dt.strftime(\"02.02.2022 %H:%M:%S\"), format=\"%d.%m.%Y %H:%M:%S\")\n",
    "df_sp[\"start_date\"] = df_sp[\"start\"].dt.date\n",
    "df_sp[\"stop_time\"]  = df_sp[\"start_time\"] + df_sp[\"duration\"]\n",
    "df_sp[\"text\"]       = df_sp[\"start\"].dt.strftime(\"%d.%m.%Y %H.%M.%S\") + \" - \" + df_sp[\"stop\"].dt.strftime(\"%d.%m.%Y %H.%M.%S\")\n",
    "                                                 \n",
    "px.timeline(df_sp, x_start=\"start_time\", x_end=\"stop_time\", y=\"start_date\", color=\"Error\", height=1200, hover_name=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbb13b",
   "metadata": {},
   "source": [
    "Die obrige Abbildung zeigt die Betriebszeiten an der y-Achse sind die Tage aufgezeigt und auf der X-Achse die Betreibszeit.\n",
    "Die Fehler wurden Farbig hervorgehoben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some space on ram\n",
    "del [[df_sp]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdccb40d",
   "metadata": {},
   "source": [
    "##### Korrelation der Daten mit Fehlern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d158b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_cols    = [c for c in df.columns if c not in [\"timestamp\", \"isError\", \"Error\", \"start_stop\"]]\n",
    "df_corr_ft = df[ft_cols].copy()\n",
    "df_corr_lb = df[[\"Error\", \"isError\"]].copy()\n",
    "\n",
    "df_corr_lb[\"IsError1\"] = df_corr_lb[\"Error\"] == 1\n",
    "df_corr_lb[\"IsError2\"] = df_corr_lb[\"Error\"] == 2\n",
    "df_corr_lb[\"IsError3\"] = df_corr_lb[\"Error\"] == 3\n",
    "\n",
    "df_corr = pd.DataFrame({e:df_corr_ft.corrwith(df_corr_lb[e]) for e in [\"isError\", \"IsError1\", \"IsError2\", \"IsError3\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fe9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(df_corr.unstack().reset_index(), x=\"level_1\", y=0, color=\"level_0\", barmode=\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b6c1e",
   "metadata": {},
   "source": [
    "Die obere Abbildung zeigt die Die Korrelation der Einzelnen Sensordaten zu den Fehlern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f8663",
   "metadata": {},
   "source": [
    "Die 3 Aussagekräftigsten Sensoren für jeden Fehler sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb787647",
   "metadata": {},
   "outputs": [],
   "source": [
    "eralg = df_corr[\"isError\"].abs().sort_values(ascending=False)\n",
    "er1 = df_corr['IsError1'].abs().sort_values(ascending=False)\n",
    "er2 = df_corr['IsError2'].abs().sort_values(ascending=False)\n",
    "er3 = df_corr['IsError3'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(f'Fehler 1: \\n{er1[:3]}\\n\\nFehler 2: \\n{er2[:3]}\\n\\nFehler 3: \\n{er3[:3]}\\n\\nAllgemein: \\n{eralg[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1790a13",
   "metadata": {},
   "source": [
    "##### Visualisierung Aller Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833389e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes two timestamps as input and visulizes the data between them\n",
    "def plot_df(df, start, end):\n",
    "    # Create a new dataframe with the data between start and end timestamps\n",
    "    mask = (df['timestamp'] > start) & (df['timestamp'] <= end)\n",
    "    df_plot = df.loc[mask]\n",
    "    # Create a plotly figure\n",
    "    fig = px.line(df_plot, x='timestamp', y='LPS', title='LPS')\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "# A function that takes two timestamps as input and visulizes the data between them using matplotlib\n",
    "def plot_df_matplotlib(df : DataFrame, start, end, column, highlightStart: dt.datetime, highlightEnd: dt.datetime):\n",
    "    # Create a new dataframe with the data between start and end timestamps\n",
    "    mask = (df['timestamp'] > start) & (df['timestamp'] <= end)\n",
    "    df_plot = df.loc[mask]\n",
    "    # Create a matplotlib figure\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_plot['timestamp'], df_plot[column])\n",
    "    ax.set(xlabel='timestamp', ylabel=column, title=column)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "    ax.grid()\n",
    "    formatter = DateFormatter('%H:%M:%S')\n",
    "    plt.axvspan(highlightStart, highlightEnd, color='red', alpha=0.5)\n",
    "    fig1 = plt.gcf()\n",
    "    fig1.axes[0].xaxis.set_major_formatter(formatter)  # Set the x-axis to display time\n",
    "    fig1.set_size_inches(18.5, 2.5)\n",
    "    plt.show()\n",
    "\n",
    "# a function that plots every column in a dataframe in a single plot using matplotlib in subplots\n",
    "def plot_df_all_columns(df, start, end):\n",
    "    # Create a new dataframe with the data between start and end timestamps\n",
    "    mask = (df['timestamp'] > start) & (df['timestamp'] <= end)\n",
    "    df_plot = df.loc[mask]\n",
    "    # Create a matplotlib figure\n",
    "    fig, axs = plt.subplots(df_plot.columns.size, 1, figsize=(20, 40))\n",
    "\n",
    "    i = 0\n",
    "    for col in df_plot.columns:\n",
    "        axs[i].plot(df_plot['timestamp'], df_plot[col])\n",
    "        axs[i].set(xlabel='timestamp', ylabel=col, title=col)\n",
    "        #display 20 x-axis labels\n",
    "        axs[i].xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "        axs[i].grid()\n",
    "        formatter = DateFormatter('%H:%M:%S')\n",
    "        fig1 = plt.gcf()\n",
    "        fig1.axes[0].xaxis.set_major_formatter(formatter)\n",
    "        i = i + 1\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898fd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aus Tabelle\n",
    "err_1_start = dt.datetime(2022,2,28,21,53)\n",
    "err_1_end = dt.datetime(2022,3,1,2,00)\n",
    "err_2_start = dt.datetime(2022,3,23,14,54)\n",
    "err_2_end = dt.datetime(2022,3,23,15,24)\n",
    "err_3_start = dt.datetime(2022,5,30,12,00)\n",
    "err_3_end = dt.datetime(2022,6,2,6,18)\n",
    "\n",
    "s = err_1_start - dt.timedelta(hours=3*4, seconds=100)\n",
    "e = err_1_end   + dt.timedelta(hours=3*4)\n",
    "plot_df_all_columns(df, s, e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71b9d202",
   "metadata": {},
   "source": [
    "### Aufgabe 1 (Klassifikation des Systemzustands)\n",
    "Erstellung eines binären Klassifikationsmodell zur Vorhersage des aktuellen Zutands der APU (Air Production Unit) auf Basis der Sensordaten. Soll differenziert werden ob APU in Ordnung oder nicht ist (binäre Klassifikation)\n",
    "zu verwendene Verfahren:\n",
    "- einen nicht-parametrisierten Modellansatz,\n",
    "- einen parametrisierten Modellansatz,\n",
    "- ein Verfahren aus dem Bereich des Ensemble Learning.\n",
    "\n",
    "Trainigsdatensatz erstellen z.B. durch geeignete Datentransformationen, Feature Engineering und ggf. Feature Extraction. Sequentielle Struktur der Daten soll berücksichtigt werden.\n",
    "Wenden Sie zur Modellerstellung (in dieser und den folgenden Aufgaben) geeignete Maßnahmen und Techniken an, damit die resultierenden Modelle eine moglichst hohe Gute aufweisen und beurteilen Sie diese anhand geeigneter Kriterien. Modularisieren und automatisieren Sie Ihren Workflow, damit die einzelnen Schritte in den folgenden Aufgaben ggf.\n",
    "wiederverwendet werden konnen. Achten Sie darauf, dass Ihre Modelle auf unbekannte Daten angewendet werden konnen, die ggf. fehlende Werte enthalten, auch wenn der gegebene Datensatz keine fehlenden Werte enthalt. Welche Features erweisen sich als besonders aussagekräftig für die gegebene Aufgabenstellung?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fd8e145",
   "metadata": {},
   "source": [
    "- **TP2** - Druck am Kompressor (bar).\n",
    "- **TP3** - An der Pneumatikzentrale erzeugter Druck (bar).\n",
    "- **H1** - Ventil, das aktiviert wird, wenn der vom Druckschalter der Steuerung abgelesene Druck über dem Betriebsdruck von 10,2 bar (bar) liegt.\n",
    "- **DV_pressure** - Druck, der durch den Druckabfall entsteht, wenn die Lufttrockentürme das Wasser. Wenn er gleich Null ist, arbeitet der Kompressor unter Last (bar).\n",
    "- **Reservoirs** - Druck in den auf den Zügen installierten Luftbehältern (bar).\n",
    "- **Oil_Temperature** - Temperatur des im Kompressor vorhandenen Öls (°C).\n",
    "- **Durchflussmesser** - Der Luftdurchfluss wurde an der pneumatischen Schalttafel gemessen (m^3\n",
    "/h).\n",
    "- **Motor_Current** - Strom des Motors, der die folgenden Werte aufweisen sollte: (i) nahe 0 A, wenn der (ii) nahe bei 4 A, wenn der Kompressor im Leerlauf arbeitet, und (iii) nahe bei 7 A, wenn der Kompressor der Kompressor unter Last arbeitet (A);\n",
    "- **COMP** - Elektrisches Signal des Lufteinlassventils des Kompressors. Es ist aktiv, wenn der Kompressor keine Luft ansaugt am Kompressor, d.h. der Kompressor schaltet ab oder arbeitet entlastet.\n",
    "- **DV_electric** - Elektrisches Signal, das das Auslassventil des Verdichters steuert. Wenn es aktiv ist, bedeutet es, dass der Kompressor unter Last arbeitet, wenn es nicht aktiv ist, bedeutet es, dass der Kompressor ausgeschaltet oder entlastet ist.\n",
    "- **TOWERS** - Signal, das festlegt, welcher Turm die Luft trocknet und welcher Turm die der Luft entzogene Feuchtigkeit abführt. Wenn es nicht aktiv ist, bedeutet es, dass Turm eins in Betrieb ist, wenn es aktiv ist, bedeutet es, fass Turm zwei in Betrieb ist.\n",
    "- **MPG** - Ist für die Aktivierung des Einlassventils verantwortlich, um den Kompressor unter Last zu starten, wenn der Druck in der der APU unter 8,2 bar liegt. Folglich aktiviert er den Sensor COMP, der das gleiche Verhalten wie der MPG-Sensor.\n",
    "- **LPS** - Signal aktiviert, wenn der Druck niedriger als 7 bar ist.\n",
    "- **Pressure_switch** - Signal, das aktiviert wird, wenn ein Druck am Vorsteuerventil festgestellt wird.\n",
    "- **Oil_Level** - Der Ölstand am Verdichter ist aktiv (gleich eins), wenn der Ölstand unter den erwarteten\n",
    "Werten liegt.\n",
    "- **Caudal_impulses** - Vom Durchflussmesser erzeugtes Signal, das den Luftdurchfluss pro Sekunde.\n",
    "\n",
    "\n",
    "Was die GPS-Informationen betrifft, so wurde der Zug mit einer sekundären GPS-Antenne ausgestattet, um Folgendes zu erfassen\n",
    "folgenden Daten:\n",
    "- **gpsLong** - Längengrad-Position (°).\n",
    "- **gpsLat** - Position des Breitengrades (°).\n",
    "- **gpsSpeed** - Geschwindigkeit (km/h).\n",
    "- **gpsQuality** - Signalqualität.\n",
    "\n",
    "!!!\n",
    "Bei der APU handelt es sich um eine Systemkomponente des Zugs, die im laufenden Betrieb ver-\n",
    "schiedene wichtige Funktionen erf ̈ullt, und deren Ausfall eine sofortige Außerbetriebnahme und\n",
    "Reparatur erforderlich macht. Weiterhin werden Angaben zu drei St ̈orungsf ̈allen gemacht, die\n",
    "sich w ̈ahrend des o.g. Betrachtungszeitraums ergeignet haben. Diese k ̈onnen verwendet werden,\n",
    "um geeignete Zielvariablen f ̈ur Methoden des Supervised Learning abzuleiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91a2aaa6",
   "metadata": {},
   "source": [
    "### Zeiterfassung der Störungsfälle (für eigene Labels)\n",
    "**Fall 1:**\n",
    "- 28.2-1.03: 22:12:00 - 6:27:00\n",
    "\n",
    "**Fall 2:**\n",
    "- 24.03: 11:15:00 - 15:08:00\n",
    "\n",
    "**Fall 3:**\n",
    "- 30.05-02.06: 04:00:00 - 07:40:00 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c711a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set labels for errors again in single column\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) | \n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02482218",
   "metadata": {},
   "outputs": [],
   "source": [
    "analog_sensors = ['TP2', 'TP3', 'H1', 'DV_pressure', 'Reservoirs', 'Oil_Temperature', 'Flowmeter', 'Motor_Current']\n",
    "digitial_sensors = ['COMP', 'DV_eletric', 'Towers', 'MPG', 'LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78447e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class FeatureGeneration:\n",
    "    \"\"\"\n",
    "    Class for feature generation\n",
    "    \"\"\"\n",
    "\n",
    "    def yfft(self, y):\n",
    "        \"\"\"\n",
    "        Compute the yFFT of a signal\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array\n",
    "            Signal\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        yfft : array\n",
    "            yFFT\n",
    "        \"\"\"\n",
    "        hanning = np.hanning(len(y))\n",
    "        yf = 4*np.abs(np.fft.rfft(y*hanning))/len(y)\n",
    "        return yf\n",
    "\n",
    "    def xfft(self, blocksize, duration):\n",
    "        \"\"\"\n",
    "        Compute the xFFT of a signal\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        blocksize : int\n",
    "            Number of samples in a block\n",
    "        duration : float\n",
    "            Duration of a block in seconds\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        xfft : array\n",
    "            xFFT\n",
    "        \"\"\"\n",
    "        return np.fft.rfftfreq(blocksize, duration)\n",
    "\n",
    "    def fourier(self, blocksize, max_freq, series, name):\n",
    "        \"\"\"\n",
    "        Compute the Fourier transform of a signal\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        blocksize : int\n",
    "            Number of samples in a block\n",
    "        max_freq : float\n",
    "            Maximum frequency to consider\n",
    "        series : array\n",
    "            Signal\n",
    "        name : str\n",
    "            Name of the signal\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : DataFrame\n",
    "            Fourier transform\n",
    "        \"\"\"\n",
    "\n",
    "        #number of blocks\n",
    "        nblocks = int(len(series)/blocksize)\n",
    "\n",
    "        data = []\n",
    "\n",
    "        X_f = self.xfft(blocksize, 1)\n",
    "        X_f = X_f[X_f<max_freq]\n",
    "        for i in range(nblocks):\n",
    "            sr_block = series[i*blocksize:(i+1)*blocksize]\n",
    "            # iserr    = df_block['Label'].sum() > BLOCKSIZE/2\n",
    "            fft_amp  = self.yfft(sr_block)[:len(X_f)]\n",
    "            # block_data = {'iserr': iserr}\n",
    "            data.append({f'{name}_{j}':amp for j,amp in enumerate(fft_amp)})\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def getError(self, blocksize, series, name, label_most_common=True):\n",
    "        \"\"\"\n",
    "        Compute the errors of a series in blocks\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        blocksize : int\n",
    "            Number of samples in a block\n",
    "        series : array\n",
    "            Error Series\n",
    "        name : str\n",
    "            Name of the Error column in the resulting DataFrame\n",
    "        label_most_common : bool, optional\n",
    "            If True, the most common value in the block is used as label. If False, the mean is used. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : DataFrame\n",
    "            Error Series\n",
    "        \"\"\"\n",
    "        #number of blocks\n",
    "        nblocks = int(len(series)/blocksize)\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for i in range(nblocks):\n",
    "            sr_block = series[i*blocksize:(i+1)*blocksize]\n",
    "\n",
    "            #iserr    = sr_block.sum() > blocksize/2\n",
    "            if(label_most_common):\n",
    "                iserr = Counter(sr_block).most_common(1)[0][0]\n",
    "            else:\n",
    "                iserr = sr_block.mean()\n",
    "            # block_data = {'iserr': iserr}\n",
    "            data.append({f'{name}':iserr})\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def getMean(self, blocksize, series, name):\n",
    "        \"\"\"\n",
    "        Compute the mean of a series in blocks\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        blocksize : int\n",
    "            Number of samples in a block\n",
    "        series : array\n",
    "            Series\n",
    "        name : str\n",
    "            Name of the column in the resulting DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : DataFrame\n",
    "            Mean Series\n",
    "        \"\"\"\n",
    "        #number of blocks\n",
    "        nblocks = int(len(series)/blocksize)\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for i in range(nblocks):\n",
    "            sr_block = series[i*blocksize:(i+1)*blocksize]\n",
    "\n",
    "            mean    = sr_block.mean()\n",
    "            # block_data = {'iserr': iserr}\n",
    "            data.append({f'{name}':mean})\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def countPeaks(self, blocksize, series, name):\n",
    "        \"\"\"\n",
    "        Compute the number of peaks of a series in blocks\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        blocksize : int\n",
    "            Number of samples in a block\n",
    "        series : array\n",
    "            Series\n",
    "        name : str\n",
    "            Name of the column in the resulting DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : DataFrame\n",
    "            Number of peaks\n",
    "        \"\"\"\n",
    "        #number of blocks\n",
    "        nblocks = int(len(series)/blocksize)\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for i in range(nblocks):\n",
    "            sr_block = series[i*blocksize:(i+1)*blocksize]\n",
    "\n",
    "            count    = sr_block[sr_block > 0.5].count()\n",
    "            # block_data = {'iserr': iserr}\n",
    "            data.append({f'{name}':count})\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def getFeatures(self, df, blocksize = 700, max_freq = 1000, label_most_common = True):\n",
    "        \"\"\"\n",
    "        Compute the features of a DataFrame\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            DataFrame containing the signals\n",
    "        blocksize : int, optional\n",
    "            Number of samples in a block. The default is 700.\n",
    "        max_freq : int, optional\n",
    "            Maximum frequency to consider. The default is 1000.\n",
    "        label_most_common : bool, optional\n",
    "            If True, the most common value in the block is used as label. If False, the mean is used. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_features : DataFrame\n",
    "            DataFrame containing the features\n",
    "        \"\"\"\n",
    "        df_features = pd.DataFrame()\n",
    "\n",
    "        blocks = blocksize\n",
    "\n",
    "        tp3 = self.fourier(blocks,max_freq, df['TP3'], 'TP3')\n",
    "        oil = self.fourier(blocks, max_freq, df['Oil_temperature'], 'Oil')\n",
    "        res = self.fourier(blocks, max_freq, df['Reservoirs'], 'Res')\n",
    "        motCurr = self.fourier(blocks, max_freq, df['Motor_current'], 'MotCurr')\n",
    "        h1 = self.fourier(blocks, max_freq, df['H1'], 'H1')\n",
    "        LPS = self.getMean(blocks, df['LPS'], 'LPS')\n",
    "        DV_pressure_peaks = self.countPeaks(blocks, df['DV_pressure'], 'DV_pressure_peaks')\n",
    "\n",
    "        errors = self.getError(blocks, df['Label'], 'Error', label_most_common)\n",
    "\n",
    "        df_features = pd.concat([df_features, tp3, oil,errors, res, motCurr, h1, LPS, DV_pressure_peaks], axis=1)\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127cdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroBinaryClassifier:\n",
    "    \"\"\"\n",
    "    Class for the binary classification of metro data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_shortened : bool, optional\n",
    "        If True, the shortened dataset is used. The default is False.\n",
    "\n",
    "    n_components : float, optional\n",
    "        Number of components to keep in the PCA. The default is 0.95.\n",
    "\n",
    "    window_size : int, optional\n",
    "        Size of the window for the rolling operations. The default is 60.\n",
    "\n",
    "    test_size : float, optional\n",
    "        Size of the test set. The default is 0.3.\n",
    "\n",
    "    random_state : int, optional\n",
    "        Random state for all random_state operations. The default is 0.\n",
    "\n",
    "    max_depth : int, optional\n",
    "        Maximum depth of the decision tree. The default is 7.\n",
    "\n",
    "    min_samples_leaf : int, optional\n",
    "        Minimum number of samples in a leaf. The default is 10.\n",
    "\n",
    "    excluded_columns : list, optional\n",
    "        List of columns to exclude from the dataset. The default is []. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_shortened: False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=7, min_samples_leaf=10, excluded_columns=[]):\n",
    "        self.n_components = n_components\n",
    "        self.windowSize = window_size\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.excluded_columns = excluded_columns\n",
    "        self.use_shortened = use_shortened\n",
    "\n",
    "        self.scaler = None\n",
    "        self.pca = None\n",
    "\n",
    "        self.feature_columns = None\n",
    "\n",
    "        #Dataset:\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "        #classifiers\n",
    "        self.decision_tree = None\n",
    "        self.random_forest = None\n",
    "        self.linear_SVM = None\n",
    "        self.naive_bayes = None\n",
    "        self.linear_discriminant_analysis = None\n",
    "\n",
    "        #regressions\n",
    "        self.polyreg_scaled = None\n",
    "        self.logistic_regression = None\n",
    "        self.linear_regression = None\n",
    "\n",
    "\n",
    "    def prepareFourier(self, data: DataFrame, blocksize = 700, max_freq = 1000, stratify = True, label_most_common = True):\n",
    "        \"\"\"\n",
    "        Prepare the data with the Fourier transform\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : DataFrame\n",
    "            DataFrame containing the signals\n",
    "        blocksize : int, optional\n",
    "            Number of samples in a block. The default is 700.\n",
    "        max_freq : int, optional\n",
    "            Maximum frequency to consider. The default is 1000.\n",
    "        stratify : bool, optional\n",
    "            If True, the train and test sets are stratified. The default is True.\n",
    "        label_most_common : bool, optional\n",
    "            If True, the most common value in the block is used as label. If False, the mean is used. The default is True.\n",
    "        \"\"\"\n",
    "        featureGenerator = FeatureGeneration()\n",
    "        df_features = featureGenerator.getFeatures(data, blocksize = blocksize, max_freq = max_freq, label_most_common = label_most_common)\n",
    "        X = df_features.drop('Error', axis=1)\n",
    "        y = df_features['Error']\n",
    "\n",
    "        s = None\n",
    "        if stratify:\n",
    "            s=y\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=s)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "\n",
    "\n",
    "    def sliding_window(self, data: DataFrame, window_size = 60, step_size=None, label_func = None, excluded_columns = ['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality']):\n",
    "        \"\"\"\n",
    "        Creates a sliding window of size window_size for each column in data.\n",
    "        Performs the following operations on each column:\n",
    "        - mean\n",
    "        - std\n",
    "        - min\n",
    "        - max\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : DataFrame\n",
    "            The DataFrame that should be transformed.\n",
    "\n",
    "        window_size : int, optional\n",
    "            The size of the sliding window. The default is 60.\n",
    "\n",
    "        excluded_columns : list, optional\n",
    "            A list of columns that should be excluded from the sliding window. The default is ['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_rolling : DataFrame\n",
    "            The transformed DataFrame.\n",
    "        \"\"\"\n",
    "        columns = data.columns\n",
    "        columns  = [x for x in columns if x not in excluded_columns]\n",
    "        operations = ['mean', 'std', 'min', 'max', 'var']\n",
    "        if(step_size == None):\n",
    "            step_size = window_size\n",
    "\n",
    "        #generate a dict, where the keys are the column names and the values are the operations that should be performed on the column\n",
    "        operations_dict = {}\n",
    "        for column in columns:\n",
    "            operations_dict[column] = operations\n",
    "\n",
    "        def mostFrequent(row):\n",
    "            return Counter(row).most_common(1)[0][0]\n",
    "\n",
    "        if(label_func == None):\n",
    "            operations_dict['Label'] = [mostFrequent]\n",
    "        else:\n",
    "            operations_dict['Label'] = [label_func]\n",
    "\n",
    "        df_rolling = data.rolling(window_size,center=True,step=window_size).agg(operations_dict)\n",
    "\n",
    "        #drop all columns that have NaN values\n",
    "        df_rolling = df_rolling.dropna()\n",
    "\n",
    "        #flatten df_rolling\n",
    "        df_rolling.columns = ['_'.join(col) for col in df_rolling.columns]\n",
    "\n",
    "        #rename Label_<opaeration> to Label\n",
    "        if(label_func == None):\n",
    "            df_rolling = df_rolling.rename(columns={'Label_mostFrequent': 'Label'})\n",
    "        else:\n",
    "            name = \"Label_\"\n",
    "            #check if the label_func is a string\n",
    "            if(isinstance(label_func, str)):\n",
    "                name += label_func\n",
    "\n",
    "            #check if the label_func is a function\n",
    "            elif(callable(label_func)):\n",
    "                name += label_func.__name__\n",
    "\n",
    "            #if the label_func is neither a string nor a function, raise an exception\n",
    "            else:\n",
    "                raise Exception(\"label_func is not a string or a function\")\n",
    "\n",
    "            df_rolling = df_rolling.rename(columns={name: 'Label'})\n",
    "        \n",
    "        return df_rolling\n",
    "    \n",
    "    def scale(self, train, test):\n",
    "        \"\"\"\n",
    "        Scale the data using StandardScaler\n",
    "        Fits the scaler on the training data and transforms the training and test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train : DataFrame\n",
    "            The training data.\n",
    "\n",
    "        test : DataFrame\n",
    "            The test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_rescaled : DataFrame\n",
    "            The rescaled training data.\n",
    "        \"\"\"\n",
    "        self.scaler = MinMaxScaler()\n",
    "        train_rescaled = self.scaler.fit_transform(train)\n",
    "        test_rescaled = self.scaler.transform(test)\n",
    "\n",
    "        return train_rescaled, test_rescaled\n",
    "    \n",
    "    def performPca(self, train, test):\n",
    "        \"\"\"\n",
    "        Perform PCA on the data.\n",
    "        Fits the PCA on the training data and transforms the training and test data.\n",
    "        This is done to reduce the dimensionality of the data.\n",
    "        This will slightly reduce the accuracy of the model, but will increase the speed of the model.\n",
    "        Uses the n_components parameter given by in the constructor of the class to determine the number of components.\n",
    "        (if a float is given, the number of components is determined by the explained variance)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train : DataFrame\n",
    "            The training data.\n",
    "\n",
    "        test : DataFrame\n",
    "            The test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_pca : DataFrame\n",
    "            The PCA transformed training data.\n",
    "\n",
    "        test_pca : DataFrame\n",
    "            The PCA transformed test data.\n",
    "        \"\"\"\n",
    "        self.pca = PCA(n_components=self.n_components,random_state=self.random_state)\n",
    "        train_pca = self.pca.fit_transform(train)\n",
    "        test_pca = self.pca.transform(test)\n",
    "\n",
    "        return train_pca, test_pca\n",
    "\n",
    "    \n",
    "    def prepareData(self, dataFrame, do_scale=True, do_pca=True,do_sliding=True, label_func=None):\n",
    "        \"\"\"\n",
    "        Prepare the data for the model.\n",
    "        This includes(if desired):\n",
    "        - sliding window\n",
    "        - scaling\n",
    "        - PCA\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataFrame : DataFrame\n",
    "            The DataFrame that should be prepared.\n",
    "\n",
    "        do_scale : bool, optional\n",
    "            Whether the data should be scaled. The default is True.\n",
    "\n",
    "        do_pca : bool, optional\n",
    "            Whether PCA should be performed on the data. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train : DataFrame\n",
    "            The training data.\n",
    "\n",
    "        X_test : DataFrame\n",
    "            The test data.\n",
    "\n",
    "        y_train : DataFrame\n",
    "            The training labels.\n",
    "\n",
    "        y_test : DataFrame\n",
    "            The test labels.\n",
    "        \"\"\"\n",
    "\n",
    "        data = dataFrame\n",
    "\n",
    "        if(self.use_shortened):\n",
    "            data = self.shorten(dataFrame)\n",
    "\n",
    "        if(do_sliding):\n",
    "            data_prepared = self.sliding_window(data, window_size=self.windowSize, excluded_columns=self.excluded_columns,label_func=label_func)\n",
    "        else:\n",
    "            data_prepared = data\n",
    "\n",
    "        y = data_prepared['Label']\n",
    "\n",
    "        if(not do_sliding):\n",
    "            ex = self.excluded_columns\n",
    "            ex.remove('Label')\n",
    "            data_prepared = data_prepared.drop(columns=ex, axis=1)\n",
    "            \n",
    "        data_prepared.drop(['Label'], axis=1, inplace=True)\n",
    "\n",
    "        X = data_prepared\n",
    "\n",
    "        self.feature_columns = X.columns\n",
    "\n",
    "        print(self.feature_columns)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "\n",
    "        \n",
    "        if(do_scale):\n",
    "            X_train, X_test = self.scale(X_train, X_test)\n",
    "\n",
    "        if(do_pca):\n",
    "            X_train, X_test = self.performPca(X_train, X_test)\n",
    "\n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def prepareDataManual(self, dataFrame, do_scale=True, do_pca=True, scatter=False, segments=5, label_func=None):\n",
    "        \"\"\"\n",
    "        Manual data preparation.\n",
    "        The data for each error and normal opeation is split into a number of segments.\n",
    "        The segments are then equally distributed over the train and test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataFrame : DataFrame\n",
    "            The DataFrame that should be prepared.\n",
    "\n",
    "        do_scale : bool, optional\n",
    "            Whether the data should be scaled. The default is True.\n",
    "\n",
    "        do_pca : bool, optional\n",
    "            Whether PCA should be performed on the data. The default is True.\n",
    "\n",
    "        scatter : bool, optional\n",
    "            Whether the data should be scattered over the train and test data. The default is False.\n",
    "\n",
    "        segments : int, optional\n",
    "            The number of segments the data should be split into. The default is 5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train : DataFrame\n",
    "            The training data.\n",
    "\n",
    "        X_test : DataFrame\n",
    "            The test data.\n",
    "\n",
    "        y_train : DataFrame\n",
    "            The training labels.\n",
    "\n",
    "        y_test : DataFrame\n",
    "            The test labels.\n",
    "        \"\"\"\n",
    "\n",
    "        #this data preperation uses manually selected train end test data instead of the train_test_split function\n",
    "        #this is done so we can still use the sliding window with a step size of 1\n",
    "\n",
    "        #the test slices are always connected to each other\n",
    "        #both train and test should have date for all 3 errors and about the same ammount of normal operation present\n",
    "        \n",
    "        #get the indices of the errors starts and ends\n",
    "        err_1_start_index = df.index[df['timestamp'] == err_1_start].tolist()[0]\n",
    "        err_1_end_index = df.index[df['timestamp'] == err_1_end].tolist()[0]\n",
    "        err_2_start_index = df.index[df['timestamp'] == err_2_start].tolist()[0]\n",
    "        err_2_end_index = df.index[df['timestamp'] == err_2_end].tolist()[0]\n",
    "        err_3_start_index = df.index[df['timestamp'] == err_3_start].tolist()[0]\n",
    "        err_3_end_index = df.index[df['timestamp'] == err_3_end].tolist()[0]\n",
    "\n",
    "        test_df = pd.DataFrame()\n",
    "        train_df = pd.DataFrame()\n",
    "\n",
    "        if(scatter):\n",
    "            #device each error in n segments for both train and test\n",
    "\n",
    "            #get the lenght of each error\n",
    "            err_1_len = err_1_end_index - err_1_start_index\n",
    "            err_2_len = err_2_end_index - err_2_start_index\n",
    "            err_3_len = err_3_end_index - err_3_start_index\n",
    "\n",
    "            #split the errors in n*2 segments and iterate over them. add every even segment to the test data and every odd segment to the train data\n",
    "            for i in range(0, segments):\n",
    "                from_idx_1 = int(err_1_start_index + err_1_len/segments * i     )\n",
    "                to_idx_1   = int(err_1_start_index + err_1_len/segments * (i+1) )\n",
    "                from_idx_2 = int(err_2_start_index + err_2_len/segments * i     )\n",
    "                to_idx_2   = int(err_2_start_index + err_2_len/segments * (i+1) )\n",
    "                from_idx_3 = int(err_3_start_index + err_3_len/segments * i     )\n",
    "                to_idx_3   = int(err_3_start_index + err_3_len/segments * (i+1) )\n",
    "                if(i % 2 == 0):\n",
    "                    #add to test\n",
    "                    test_df = test_df.append(self.sliding_window(df[from_idx_1:to_idx_1],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "                    test_df = test_df.append(self.sliding_window(df[from_idx_2:to_idx_2],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "                    test_df = test_df.append(self.sliding_window(df[from_idx_3:to_idx_3],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "                else:\n",
    "                    #add to train\n",
    "                    train_df = train_df.append(self.sliding_window(df[from_idx_1:to_idx_1],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "                    train_df = train_df.append(self.sliding_window(df[from_idx_2:to_idx_2],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "                    train_df = train_df.append(self.sliding_window(df[from_idx_3:to_idx_3],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "\n",
    "            #add the normal operation data to the train and test data\n",
    "            before_err_1 = df[0 : err_1_start_index-1]\n",
    "            before_err_2 = df[err_1_end_index+1 : err_2_start_index-1]\n",
    "            before_err_3 = df[err_2_end_index+1 : err_3_start_index-1]\n",
    "\n",
    "            #add the normal operation data to the train and test data\n",
    "            for i in range(0,segments):\n",
    "                from_idx_1  = int(before_err_1.shape[0]/segments * i        )\n",
    "                to_idx_1    = int(before_err_1.shape[0]/segments * (i+1)    )\n",
    "                from_idx_2  = int(before_err_2.shape[0]/segments * i        )\n",
    "                to_idx_2    = int(before_err_2.shape[0]/segments * (i+1)    )\n",
    "                from_idx_3  = int(before_err_3.shape[0]/segments * i        )\n",
    "                to_idx_3    = int(before_err_3.shape[0]/segments * (i+1)    )\n",
    "\n",
    "                if(i % 2 == 0):\n",
    "                    #add to test\n",
    "                    test_df = test_df.append(self.sliding_window(before_err_1[from_idx_1:to_idx_1],window_size=self.windowSize, excluded_columns=self.excluded_columns,step_size=1))\n",
    "                    test_df = test_df.append(self.sliding_window(before_err_2[from_idx_2:to_idx_2],window_size=self.windowSize, excluded_columns=self.excluded_columns,step_size=1))\n",
    "                    test_df = test_df.append(self.sliding_window(before_err_3[from_idx_3:to_idx_3],window_size=self.windowSize, excluded_columns=self.excluded_columns,step_size=1))\n",
    "                else:\n",
    "                    #add to train\n",
    "                    train_df = train_df.append(self.sliding_window(before_err_1[from_idx_1:to_idx_1],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "                    train_df = train_df.append(self.sliding_window(before_err_2[from_idx_2:to_idx_2],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "                    train_df = train_df.append(self.sliding_window(before_err_3[from_idx_3:to_idx_3],window_size=self.windowSize, excluded_columns=self.excluded_columns, step_size=1))\n",
    "\n",
    "            #reorder the dfs by index\n",
    "            train_df = train_df.sort_index()\n",
    "            test_df = test_df.sort_index()\n",
    "\n",
    "            #get labels\n",
    "            self.y_train = train_df['Label']\n",
    "            self.y_test = test_df['Label']\n",
    "            \n",
    "            train_df = train_df.drop(['Label'], axis=1)\n",
    "            test_df = test_df.drop(['Label'], axis=1)\n",
    "\n",
    "            #get the features\n",
    "            self.X_train = train_df\n",
    "            self.X_test = test_df\n",
    "            \n",
    "            return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "\n",
    "    \n",
    "    def evalModels(self):\n",
    "        \"\"\"\n",
    "        Evaluate the models.\n",
    "        Prints a Plotly interactive Bar chart for each fitted model that shows the precision, recall and f1 score for each class.\n",
    "        \"\"\"\n",
    "\n",
    "        calssifiers = [self.decision_tree, self.random_forest, self.linear_regression, self.linear_SVM, self.naive_bayes, self.linear_discriminant_analysis, self.logistic_regression]\n",
    "\n",
    "\n",
    "        results = []\n",
    "        clfs = []\n",
    "        precison = {}\n",
    "        recall = {}\n",
    "        f1_score = {}\n",
    "\n",
    "\n",
    "        for label in self.y_test.unique():\n",
    "            precison[str(label)] = []\n",
    "            recall  [str(label)] = []\n",
    "            f1_score[str(label)] = []\n",
    "\n",
    "\n",
    "        for classifier in calssifiers:\n",
    "            if(classifier is not None):\n",
    "                y_true, y_pred = self.y_test, classifier.predict(self.X_test)\n",
    "                evaluation = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "                clfs.append(classifier.__class__.__name__)\n",
    "                    \n",
    "                for label in self.y_test.unique():\n",
    "\n",
    "                    precison[str(label)].append(evaluation[str(label)]['precision'])\n",
    "                    recall  [str(label)].append(evaluation[str(label)]['recall'])\n",
    "                    f1_score[str(label)].append(evaluation[str(label)]['f1-score'])\n",
    "\n",
    "        print(precison, recall, f1_score)\n",
    "\n",
    "        print(results)\n",
    "        #make a plot with plotly, where on the x axis are  the classifiers, each classifier has bar for classes precison, recall and f1-score\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for label in self.y_test.unique():\n",
    "            data.append(go.Bar(name ='[' + str(label) + '] precision',    x=clfs, y=precison[str(label)] ,text=precison[str(label)] ,textposition='auto',))\n",
    "            data.append(go.Bar(name ='[' + str(label) + '] recall',       x=clfs, y=recall[str(label)]   ,text=recall[str(label)]   ,textposition='auto',))\n",
    "            data.append(go.Bar(name ='[' + str(label) + '] f1-score',     x=clfs, y=f1_score[str(label)] ,text=f1_score[str(label)] ,textposition='auto',))\n",
    "\n",
    "        fig = go.Figure(data=data)\n",
    "        # Change the bar mode\n",
    "        fig.update_layout(barmode='group')\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    def ConfusionMatricesis(self):\n",
    "        \"\"\"\n",
    "        Shows a plotly interactive confusion matrix for each fitted model.\n",
    "        \"\"\"\n",
    "\n",
    "        calssifiers = [self.decision_tree, self.random_forest, self.linear_regression, self.linear_SVM, self.naive_bayes, self.linear_discriminant_analysis,self.logistic_regression]\n",
    "\n",
    "        #list of all classifiers that are not None\n",
    "        clfs = [clf for clf in calssifiers if clf is not None]\n",
    "        y_true = self.y_test\n",
    "        cmats = []\n",
    "        \n",
    "        #get the confusion matrix for each classifier\n",
    "        for clf in clfs:\n",
    "            y_pred = clf.predict(self.X_test)\n",
    "            cmats.append(confusion_matrix(y_true, y_pred)) \n",
    "            \n",
    "        #Titles for each matrix is the class name of the classifier\n",
    "        titles = [clf.__class__.__name__ for clf in clfs]\n",
    "\n",
    "        #Calculate the optimal grid size for the plot\n",
    "        n = len(clfs)\n",
    "        rows = int(math.sqrt(n))\n",
    "        cols = int(math.ceil(n / rows))\n",
    "        \n",
    "        #plotgird for the confusion matrices\n",
    "        fig = make_subplots(rows=rows, cols=cols,\n",
    "                        subplot_titles=(titles))\n",
    "\n",
    "        \n",
    "        #add the confusion matrices to the plot\n",
    "        r=1\n",
    "        c=1\n",
    "        for i in range(len(clfs)):\n",
    "            heatmap = go.Heatmap(\n",
    "                z=cmats[i],\n",
    "                text=cmats[i],\n",
    "                texttemplate=\"%{text}\",\n",
    "                textfont={\"size\": 10},)            \n",
    "\n",
    "            fig.add_trace(heatmap,row=r,col=c)\n",
    "            \n",
    "            #fill the grid from left to right, top to bottom\n",
    "            if(c<cols):\n",
    "                c=c+1\n",
    "            else:\n",
    "                r=r+1\n",
    "                c=1\n",
    "        fig.update_layout(title = \"Confusion Matrices\")\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    def fitDecisionTree(self):\n",
    "        self.decision_tree = DecisionTreeClassifier()\n",
    "        self.decision_tree.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def decisionTreeConfusionMatrix(self):\n",
    "        y_pred = self.decision_tree.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.decision_tree.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def fitRandomForest(self):\n",
    "        self.random_forest = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
    "        self.random_forest.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def randomForestConfusionMatrix(self):\n",
    "        y_pred = self.random_forest.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.random_forest.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def fitLinearRegression(self):\n",
    "        self.linear_regression = LinearRegression(n_jobs=-1)\n",
    "        self.linear_regression.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def linearRegressionConfusionMatrix(self):\n",
    "        y_pred = self.linear_regression.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.linear_regression.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def fitLinearSVM(self):\n",
    "        self.linear_SVM = LinearSVC()\n",
    "        self.linear_SVM.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def fitNaiveBayes(self):\n",
    "        self.naive_bayes = GaussianNB(var_smoothing=1e-15,)\n",
    "        self.naive_bayes.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def naiveBayesConfusionMatrix(self):\n",
    "        y_pred = self.naive_bayes.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.naive_bayes.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def fitLinearDiscriminantAnalysis(self):\n",
    "        self.linear_discriminant_analysis = LinearDiscriminantAnalysis()\n",
    "        self.linear_discriminant_analysis.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def linearDiscriminantAnalysisConfusionMatrix(self):\n",
    "        y_pred = self.linear_discriminant_analysis.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.linear_discriminant_analysis.classes_)\n",
    "        return disp.plot()\n",
    "    \n",
    "    def linearDiscriminantAnalysisFindSolver(self):\n",
    "        # define model\n",
    "        model = LinearDiscriminantAnalysis()\n",
    "        # define model evaluation method\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "        # define grid\n",
    "        grid = dict()\n",
    "        grid['solver'] = ['svd', 'lsqr', 'eigen']\n",
    "        # define search\n",
    "        search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        # perform the search\n",
    "        results = search.fit(self.X_train, self.y_train)\n",
    "        # summarize\n",
    "        print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "        print('Config: %s' % results.best_params_)\n",
    "    \n",
    "    def linearSVMConfusionMatrix(self):\n",
    "        y_pred = self.linear_SVM.predict(self.X_test)\n",
    "        y_true = self.y_test\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.linear_SVM.classes_)\n",
    "        return disp.plot()\n",
    "\n",
    "    def fitPlynomialRegression(self,degree=9):\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        self.polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "        self.polyreg_scaled.fit(self.X_train,self.y_train) \n",
    "        \n",
    "    #TODO: ÄNDERN\n",
    "    def shorten(self, df):\n",
    "        \"\"\"\n",
    "        Shorten the dataframe around the failures. This should be done to test effects of changes on the models, but shpouldnt be done for the final model.\n",
    "        \"\"\"\n",
    "        dfMin = df.copy()\n",
    "        #drop vor Fail 1\n",
    "        dfMin.drop(dfMin[dfMin['timestamp'] < '2022-02-26 23:00:00'].index,inplace=True)\n",
    "\n",
    "        #drop zwischen Fail1 und Fail2\n",
    "        dfMin.drop(dfMin[(dfMin['timestamp'] > '2022-03-03 00:00:00') & (dfMin['timestamp'] <= '2022-03-21 00:00:00')].index,inplace=True)\n",
    "\n",
    "        #drop zwischen Fail 2 und Fail 3\n",
    "        dfMin.drop(dfMin[(dfMin['timestamp'] > '2022-03-25 00:00:00') & (dfMin['timestamp'] <= '2022-05-28 00:00:00')].index,inplace=True)\n",
    "\n",
    "        dfMin['Label'].value_counts()\n",
    "        \n",
    "        return dfMin\n",
    "\n",
    "    def fitLogisticRegression(self, class_weight='balanced', max_iter=1000):\n",
    "        self.logistic_regression = LogisticRegression(class_weight=class_weight, random_state=self.random_state, max_iter=max_iter)\n",
    "        self.logistic_regression.fit(self.X_train, self.y_train)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clf = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "temp_clf.prepareFourier(df)\n",
    "\n",
    "print(\"Fit Decision Tree\")\n",
    "temp_clf.fitDecisionTree()\n",
    "print(\"Fit Random Forest\")\n",
    "temp_clf.fitRandomForest()\n",
    "print(\"Fit Naive Bayes\")\n",
    "temp_clf.fitNaiveBayes()\n",
    "print(\"Fit Linear Discriminant Analysis\")\n",
    "temp_clf.fitLinearDiscriminantAnalysis()\n",
    "print(\"Fit Linear SVM\")\n",
    "temp_clf.fitLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e009b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clf.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clf.ConfusionMatricesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a719815",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = temp_clf.random_forest.feature_importances_\n",
    "forest = temp_clf.random_forest\n",
    "forest_importances = pd.Series(importances, index=temp_clf.feature_columns)\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(30)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7489e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clf.X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clf = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe474996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "temp_clf.prepareDataManual(df,scatter=True, segments=4)\n",
    "print(temp_clf.y_train.shape)\n",
    "print(temp_clf.y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39babfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clf.fitDecisionTree()\n",
    "temp_clf.fitRandomForest()\n",
    "temp_clf.fitNaiveBayes()\n",
    "temp_clf.fitLinearDiscriminantAnalysis()\n",
    "temp_clf.fitLogisticRegression()\n",
    "\n",
    "temp_clf.ConfusionMatricesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f97c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clf.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84ad75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d745122",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "print(\"Prepareing Data...\")\n",
    "_, _, _, _ = mb.prepareFourier(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting Decision Tree...\")\n",
    "mb.fitDecisionTree()\n",
    "print(\"Fitting Random Forest...\")\n",
    "mb.fitRandomForest()\n",
    "print(\"Fitting Linear Regression...\")\n",
    "mb.fitLinearDiscriminantAnalysis()\n",
    "print(\"Fitting Naive Bayes...\")\n",
    "mb.fitNaiveBayes()\n",
    "print(\"Fitting Logisitic Regression...\")\n",
    "mb.fitLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4085860",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e412bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.randomForestConfusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.ConfusionMatricesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.fitLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.logistic_regression.score(mb.X_test, mb.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mb.logistic_regression.predict(mb.X_test)\n",
    "log_df = pd.DataFrame({'Actual': mb.y_test, 'Predicted': pred})\n",
    "cm = confusion_matrix(log_df['Actual'], log_df['Predicted'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mb.logistic_regression.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabf996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all but the excluded columns\n",
    "excluded = ['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality']\n",
    "columns = [c for c in df.columns if c not in excluded]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f0282ff",
   "metadata": {},
   "source": [
    "Welche Features sind besonders Aussagekräftig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7544ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = mb.random_forest.feature_importances_\n",
    "forest = mb.random_forest\n",
    "forest_importances = pd.Series(importances, index=mb.feature_columns)\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(30)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb41550",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.lossFunctionDecisionTree2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2b43d",
   "metadata": {},
   "source": [
    "Besonder Aussagekräftig sind Vorallem die Analogen Sensoren\n",
    "Vergleich training nur mit Analog Sensoren und mit allen Sensoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceeca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = ['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality']\n",
    "excluded.extend(digitial_sensors)\n",
    "#remove duplicates\n",
    "excluded = list(dict.fromkeys(excluded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metClf = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=900, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=excluded)\n",
    "print(\"Prepareing Data...\")\n",
    "_, _, _, _ = metClf.prepareData(df, do_scale=True, do_pca=False)\n",
    "\n",
    "#fit classifiers\n",
    "print(\"Fitting Decision Tree...\")\n",
    "metClf.fitDecisionTree()\n",
    "print(\"Fitting Random Forest...\")\n",
    "metClf.fitRandomForest()\n",
    "print(\"Fitting Linear Regression...\")\n",
    "metClf.fitLinearDiscriminantAnalysis()\n",
    "print(\"Fitting Naive Bayes...\")\n",
    "metClf.fitNaiveBayes()\n",
    "\n",
    "metClf.evalModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ff5b8",
   "metadata": {},
   "source": [
    "Ergebniss: Ohne Digitale Sensoren Modell minimal Besser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = metClf.random_forest.feature_importances_\n",
    "forest = metClf.random_forest\n",
    "forest_importances = pd.Series(importances, index=metClf.feature_columns)\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(30)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the 10 most importent features in descending order\n",
    "forest_importances.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ERKLÄRUNGEN HINZUFÜGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e3748",
   "metadata": {},
   "source": [
    "Werden nur Analoge Sensoren Verwendet, sind die Wichtigsten Sensoren:\n",
    "- **DV_pressure_mean**      [0.087352]\n",
    "- **DV_pressure_min**       [0.072631]\n",
    "- **Reservoirs_max**        [0.060411]\n",
    "- **Flowmeter_min**         [0.055333]\n",
    "- **Reservoirs_min**        [0.051933]\n",
    "- **TP3_std**               [0.050655]\n",
    "- **H1_std**                [0.049547]\n",
    "- **Oil_temperature_max**   [0.048480]\n",
    "- **Oil_temperature_mean**  [0.046990]\n",
    "- **Reservoirs_mean**       [0.046393]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b48998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c993957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #find best parameters for max_depth and min_samples_leaf on a range from 3 to 30 for best accuracy\n",
    "\n",
    "# best_accuracy = 0\n",
    "# best_params = (0, 0)\n",
    "\n",
    "# #column of the label\n",
    "# label = 'Label'\n",
    "# #only keep columns: TP3, LPS, DV_pressure, Oil_temperature\n",
    "# columns = ['TP3', 'LPS', 'DV_pressure', 'Oil_temperature']\n",
    "\n",
    "# for max_depth in range(3,15):\n",
    "#     for min_samples_leaf in range(3,12):\n",
    "#         clf = DecisionTreeBinaryClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, columns=columns)\n",
    "#         clf.fit(df=df, label=label)\n",
    "#         accuracy = clf.score()\n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_params = (max_depth, min_samples_leaf)\n",
    "\n",
    "# print(\"Best max_depth:\", best_params[0])\n",
    "# print(\"Best min_samples_leaf:\", best_params[1])\n",
    "# print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ea1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bc90c5b",
   "metadata": {},
   "source": [
    "### Aufgabe 2 (Vorhersage des Eintretens von Störungen)\n",
    "Erstellen Sie nun Klassifikationsmodelle, um anhand der gegebenen Sensormessdaten vorherzusagen, ob innerhalb eines bestimmten Zeitraums (z.B. 1 Stunde, 2 Stunden etc.) eine Storung der APU auftreten wird. Laut Betreiber ware es wünschenswert, mindestens zwei Stunden im Voraus eine Storung vorhersagen zu können, um rechtzeitig Maßnahmen einzuleiten, vgl. [1]. Testen Sie verschiedene Prognosezeitraume und stellen Sie die resultierenden Modelle gegen über."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e68f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEE:\n",
    "# Datensatz shiften um ein gewisses Window, labels aber gleich behalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e322307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single step is 1s \n",
    "shiff_steps = 60*60*12 # 1a011\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Create a new dataframe, that has a extra column foreach column with the vlaue of the original column shifted by \"shiff_steps\" steps\n",
    "df_shifted = pd.DataFrame()\n",
    "for column in df.columns:\n",
    "    if column != 'Label' or column != 'timestamp':\n",
    "        df_shifted[column] = df[column].shift(shiff_steps)\n",
    "        #df_shifted[column+\"(t)\"] = df[column]\n",
    "\n",
    "    if(column == 'Label' or column == 'timestamp'):\n",
    "        df_shifted[column] = df[column]\n",
    "\n",
    "#drop nan rows\n",
    "df_shifted.dropna(inplace=True)\n",
    "\n",
    "df_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7babc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "#show the shifted_data around the second failure with a buffer of 3 hours before and after\n",
    "df_shifted[(df_shifted['timestamp'] > '2022-03-23 15:00:00') & (df_shifted['timestamp'] <= '2022-03-25 17:00:00')]\n",
    "#df[(df['timestamp'] > '2022-03-23 15:00:00') & (df['timestamp'] <= '2022-03-25 17:00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb94eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "futureBinaryClassifier = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "futureBinaryClassifier.prepareFourier(df_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f490f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "futureBinaryClassifier.fitRandomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(futureBinaryClassifier.randomForestScore())\n",
    "futureBinaryClassifier.randomForestConfusionMatrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0cc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = futureBinaryClassifier.random_forest.feature_importances_\n",
    "forest = futureBinaryClassifier.random_forest\n",
    "forest_importances = pd.Series(importances, index=forest.feature_names_in_)\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MetroBinaryClassifier(use_shortened=True, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "\n",
    "#X_train, X_test, y_train, y_test = m.prepareData(df_shifted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a399ad40",
   "metadata": {},
   "source": [
    "### Aufgabe 3 (Vorhersage der Dauer von Störungen)\n",
    "Entwickeln Sie Prognosemodelle zur Vorhersage der Storungsdauer und beurteilen Sie auf geeignete Weise deren Gute sowie deren Eignung für den Einsatz in der Praxis. Sofern diese aus Ihrer Sicht nicht ausreichend ist, skizzieren Sie Maßnahmen, durch die die Gute verbessert werden konnte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aus Tabelle\n",
    "err_1_start = dt.datetime(2022,2,28,21,53)\n",
    "err_1_end = dt.datetime(2022,3,1,2,00)\n",
    "err_2_start = dt.datetime(2022,3,23,14,54)\n",
    "err_2_end = dt.datetime(2022,3,23,15,24)\n",
    "err_3_start = dt.datetime(2022,5,30,12,00)\n",
    "err_3_end = dt.datetime(2022,6,2,6,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methode 1: Labels sind verbleibende Störungszeit\n",
    "\n",
    "df_remaining = pd.read_csv('dataset_train.csv')\n",
    "df_remaining['timestamp'] = pd.to_datetime(df_remaining['timestamp'])\n",
    "\n",
    "#Set Labels. The label is the remaining time until the end of the current failure. if there is no failure, the label is 0\n",
    "\n",
    "def LabelGeneration(row):\n",
    "    timestamp = row['timestamp']\n",
    "\n",
    "    #Fail 1\n",
    "    if timestamp > err_1_start and timestamp <= err_1_end:\n",
    "        return (err_1_end - timestamp).total_seconds()\n",
    "\n",
    "    #Fail 2\n",
    "    if timestamp > err_2_start and timestamp <= err_2_end:\n",
    "        return (err_2_end - timestamp).total_seconds()\n",
    "\n",
    "    #Fail 3\n",
    "    if timestamp > err_3_start and timestamp <= err_3_end:\n",
    "        return (err_3_end - timestamp).total_seconds()\n",
    "\n",
    "    #No Fail\n",
    "    return 0\n",
    "    \n",
    "df_remaining['Label'] = df_remaining.apply(lambda row: LabelGeneration(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining[(df_remaining['timestamp'] > '2022-03-23 15:00:00') & (df_remaining['timestamp'] <= '2022-03-23 15:24:00')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the labels\n",
    "df_remaining['Label'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "X_train, X_test, y_train, y_test =  c.prepareFourier(df_remaining, 512, 0.1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827d2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "randReg = RandomForestRegressor(n_estimators=40, max_depth=20, random_state=0, n_jobs=-1)\n",
    "\n",
    "randReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543792c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = randReg.predict(X_test)\n",
    "\n",
    "results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "results.sort_index(inplace=True)\n",
    "\n",
    "#plot with plotly\n",
    "fig = px.line(results, x=results.index, y=[\"Actual\", \"Predicted\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0270ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.fitLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.linear_regression.predict(c.X_test)\n",
    "actual = c.y_test.copy()\n",
    "#reset the index of the actual values\n",
    "results = pd.DataFrame({'Actual': actual, 'Predicted': c.linear_regression.predict(c.X_test)})\n",
    "results.sort_index(inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot with plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=results.index, y=results['Actual'], name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=results.index, y=results['Predicted'], name='Predicted'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f801b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #find best accuracy\n",
    "# for i in range(1, 20):\n",
    "#     c.fitPlynomialRegression(degree=i)\n",
    "#     print('degree: ', i, ' score: ', c.polyregScore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f166bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.fitPlynomialRegression(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.polyregScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f657f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = c.polyreg_scaled.predict(c.X_test)\n",
    "\n",
    "actual = c.y_test.copy()\n",
    "#reset the index of the actual values\n",
    "actual.reset_index(drop=True, inplace=True)\n",
    "results = pd.DataFrame({'Actual': actual, 'Predicted': y_pred})\n",
    "\n",
    "plt.plot(actual, label='Actual')\n",
    "plt.plot(results['Predicted'], label='Predicted')\n",
    "plt.legend()\n",
    "#set the dimension of the plot so taht the x axis is not to small\n",
    "plt.rcParams[\"figure.figsize\"] = (40,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.polyregScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#order the results df by the actual values descending\n",
    "results.sort_values(by=['Actual'], ascending=False, inplace=True)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "plt.plot(actual, label='Actual')\n",
    "plt.plot(results['Predicted'], label='Predicted')\n",
    "plt.legend()\n",
    "#set the dimension of the plot so taht the x axis is not to small\n",
    "plt.rcParams[\"figure.figsize\"] = (40,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ee227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt with plotly\n",
    "import plotly.express as px\n",
    "fig = px.line(results, x=results.index, y=[\"Actual\", \"Predicted\"], title='Actual vs Predicted')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e588e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fae63e01",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. Ansatz: Verbleibende Zeit in Klassen aufteilen.\n",
    "Anstelle dass die Labels die verbleibende Zeit bis zum Ende der Störung sind, werden die Labels die verbleibende Zeit in Klassen aufgeteilt.\n",
    "Die Klassen sind x verbleibende Minuten. So kann ein Klassifikator auf das Problem angewand werden, alledings kann bei diesem Ansatz selbst ein perfekter klassifikator die zeit nur so genau vorhersagen wie die Klassen aufgeteilt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3aeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data again \n",
    "df_remaining = pd.read_csv('dataset_train.csv')\n",
    "\n",
    "#set the timestamp column to datetime\n",
    "df_remaining['timestamp'] = pd.to_datetime(df_remaining['timestamp'])\n",
    "\n",
    "#set the labels to 0\n",
    "df_remaining['Label'] = 0\n",
    "\n",
    "X_MINUTES = 10\n",
    "\n",
    "#now we set the labels not corresponding to the remaining seconds, but to the remaining x minutes (rounded up, so the last minute does not classify as 0)\n",
    "def LabelGeneration(row):\n",
    "    timestamp = row['timestamp']\n",
    "\n",
    "    #Fail 1\n",
    "    if timestamp > err_1_start and timestamp <= err_1_end:\n",
    "        return math.ceil((err_1_end - timestamp).total_seconds() / (60*X_MINUTES))\n",
    "\n",
    "    #Fail 2\n",
    "    if timestamp > err_2_start and timestamp <= err_2_end:\n",
    "        return math.ceil((err_2_end - timestamp).total_seconds() / (60*X_MINUTES))\n",
    "\n",
    "    #Fail 3\n",
    "    if timestamp > err_3_start and timestamp <= err_3_end:\n",
    "        return math.ceil((err_3_end - timestamp).total_seconds() / (60*X_MINUTES))\n",
    "\n",
    "    #No Fail\n",
    "    return 0\n",
    "\n",
    "df_remaining['Label'] = df_remaining.apply(lambda row: LabelGeneration(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining[(df_remaining['timestamp'] > '2022-03-23 15:00:00') & (df_remaining['timestamp'] <= '2022-03-23 15:24:00')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cef20b41",
   "metadata": {},
   "source": [
    "Jetzt können wieder die gleichen Klassifikatoren wie oben angewandt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier_fourier = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=30, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "\n",
    "_,_,_,_ = remaining_classifier_fourier.prepareFourier(df_remaining,stratify=False, blocksize=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier_sliding = MetroBinaryClassifier(use_shortened=True, n_components=0.95, window_size=10, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = remaining_classifier_sliding.prepareData(df_remaining, do_scale = True, do_sliding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier_sliding.fitRandomForest()\n",
    "y_pred = remaining_classifier_sliding.random_forest.predict(remaining_classifier_sliding.X_test)\n",
    "cm = confusion_matrix(remaining_classifier_sliding.y_test, y_pred)\n",
    "cm = cm[:,1:]\n",
    "#plot the confusion matrix\n",
    "plt.figure(figsize=(20,14))\n",
    "sns.heatmap(cm, annot=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({'Actual': remaining_classifier_sliding.y_test, 'Predicted': y_pred})\n",
    "#sort by index\n",
    "df_res.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "#plot with plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Actual'], name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Predicted'], name='Predicted'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31037aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "randomReg = RandomForestRegressor(n_estimators=40, random_state=0, n_jobs=-1)\n",
    "\n",
    "randomReg.fit(remaining_classifier_sliding.X_train, remaining_classifier_sliding.y_train)\n",
    "\n",
    "y_pred = randomReg.predict(remaining_classifier_sliding.X_test)\n",
    "\n",
    "df_res = pd.DataFrame({'Actual': remaining_classifier_sliding.y_test, 'Predicted': y_pred})\n",
    "#sort by index\n",
    "df_res.sort_index(inplace=True)\n",
    "#plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Actual'], name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Predicted'], name='Predicted'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Fitting Decision Tree..\")\n",
    "#remaining_classifier.fitDecisionTree()\n",
    "print(\"Fitting Random Forest..\")\n",
    "remaining_classifier_fourier.fitRandomForest()\n",
    "# print(\"Fitting Naive Bayes..\")\n",
    "# remaining_classifier.fitNaiveBayes()\n",
    "# print(\"Fitting Linear Regression..\")\n",
    "# remaining_classifier.fitLinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21968be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remaining_classifier.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier_fourier.random_forest.score(remaining_classifier_fourier.X_test, remaining_classifier_fourier.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5895df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = remaining_classifier_fourier.random_forest.predict(remaining_classifier_fourier.X_test)\n",
    "cm = cm[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9872406",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(remaining_classifier_fourier.y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the 0 columns from the confusion matrix(because the majority of the data is 0)\n",
    "cm = cm[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plt.figure(figsize=(20,14))\n",
    "sns.heatmap(cm, annot=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({'Actual': remaining_classifier_fourier.y_test, 'Predicted': y_pred})\n",
    "#sort by index\n",
    "df_res.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "#plot with plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Actual'], name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=df_res.index, y=df_res['Predicted'], name='Predicted'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------logistic regression----------------\n",
    "\n",
    "remaining_classifier_fourier.fitLogisticRegression(class_weight=None)\n",
    "\n",
    "remaining_classifier_fourier.logistic_regression.score(remaining_classifier_fourier.X_test, remaining_classifier_fourier.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e225801",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = remaining_classifier_fourier.logistic_regression.predict(remaining_classifier_fourier.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = pd.DataFrame({'Actual': remaining_classifier_fourier.y_test, 'Predicted': y_pred})\n",
    "#sort by index\n",
    "df_log.sort_index(inplace=True)\n",
    "\n",
    "#plot with plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_log.index, y=df_log['Actual'], name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=df_log.index, y=df_log['Predicted'], name='Predicted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca6df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd048e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier_fourier.fitPlynomialRegression(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_classifier_fourier.polyregScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee988c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit linear regression\n",
    "remaining_classifier_fourier.fitLinearRegression()\n",
    "remaining_classifier_fourier.linear_regression.score(remaining_classifier_fourier.X_test, remaining_classifier_fourier.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = remaining_classifier_fourier.linear_regression.predict(remaining_classifier_fourier.X_test)\n",
    "y_true = remaining_classifier_fourier.y_test\n",
    "\n",
    "#plot the results\n",
    "res_Df = pd.DataFrame({'Actual': y_true, 'Predicted': y_pred})\n",
    "res_Df.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "#plot with plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=res_Df.index, y=res_Df['Actual'], name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=res_Df.index, y=res_Df['Predicted'], name='Predicted'))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f224287d",
   "metadata": {},
   "source": [
    "### Aufgabe 4 (Vorhersage der gestorten Komponente) \n",
    "Untersuchen Sie, ob sich der Datensatz auch dazu eignet, die von einer Storung betroffenen Komponente anhand der Sensordaten zu identifizieren. Erstellen und evaluieren Sie dazu entsprechende Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from csv, because we need to reassing the labels\n",
    "df_type = pd.read_csv('dataset_train.csv')\n",
    "df_type['timestamp'] = pd.to_datetime(df_type['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the 3 error timespans set a label colum that from 1 to 3 and the rest to 0\n",
    "#Aus Tabelle\n",
    "err_1_start = dt.datetime(2022,2,28,21,53)\n",
    "err_1_end = dt.datetime(2022,3,1,2,00)\n",
    "err_2_start = dt.datetime(2022,3,23,14,54)\n",
    "err_2_end = dt.datetime(2022,3,23,15,24)\n",
    "err_3_start = dt.datetime(2022,5,30,12,00)\n",
    "err_3_end = dt.datetime(2022,6,2,6,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bc752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#add a label column to the dataframe that is only zeros\n",
    "df_type['Label'] = 0\n",
    "\n",
    "#if the data is between err_1_start and err_1_end set the label to 1\n",
    "df_type.loc[(df_type['timestamp'] >= err_1_start) & (df_type['timestamp'] <= err_1_end), 'Label'] = 1\n",
    "\n",
    "#if the data is between err_2_start and err_2_end set the label to 2\n",
    "df_type.loc[(df_type['timestamp'] >= err_2_start) & (df_type['timestamp'] <= err_2_end), 'Label'] = 2\n",
    "\n",
    "#if the data is between err_3_start and err_3_end set the label to 3\n",
    "df_type.loc[(df_type['timestamp'] >= err_3_start) & (df_type['timestamp'] <= err_3_end), 'Label'] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=30, test_size=0.4, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "_,_,_,_ = clf.prepareFourier(df_type,blocksize=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fitRandomForest()\n",
    "clf.fitDecisionTree()\n",
    "clf.fitNaiveBayes()\n",
    "clf.fitLinearDiscriminantAnalysis()\n",
    "clf.fitLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.logistic_regression.predict(clf.X_test)\n",
    "cm = confusion_matrix(clf.y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.logistic_regression.classes_)\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.randomForestConfusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf215bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.evalModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba25fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.ConfusionMatricesis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4555b60f",
   "metadata": {},
   "source": [
    "### Aufgabe 5 (Störungserkennung mit Hilfe von Unsupervised Learning)\n",
    "Eine Herausforderung bei der Modellbildung fur Predictive Maintenance ist häufig das Fehlen von Informationen zu historischen Storungen, sodass Ansätze des Supervised Learning nicht anwendbar sind. In diesem Fall konnen Methoden des Unsupervised Learning eine Option sein. Wenden Sie auf den Datensatz aus Aufgabe 1 (ohne Labels) ein Clustering-Verfahren an und\n",
    "uberprüfen Sie anhand der gegebenen Informationen zu den historischen Systemausfällen, ob und wie gut sich durch einen solchen Ansatz Ausnahmezustande (Anomalien bzw. Störungen) von \"normalen\" Systemzustanden der APU unterscheiden lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) | \n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5273be",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "df_sliding = p.sliding_window(df, 60)\n",
    "\n",
    "y_true = df_sliding['Label']\n",
    "df_sliding.drop(['Label'], axis=1, inplace=True)\n",
    "\n",
    "df_sliding = StandardScaler().fit_transform(df_sliding) # normalize data\n",
    "\n",
    "#df_sliding, _ = p.performPca(df_sliding, df_sliding)\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# number of clusters\n",
    "k = 2\n",
    "\n",
    "# create and fit k-means model\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(df_sliding)\n",
    "\n",
    "# predict cluster labels for new data\n",
    "labels = kmeans.predict(df_sliding)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique values of the labels array\n",
    "\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5476d8b",
   "metadata": {},
   "source": [
    "Beste Clustergröße herausfinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14367ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(df_sliding)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(1, 11), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552398eb",
   "metadata": {},
   "source": [
    "WCSS\n",
    "* 1: 2097494814.352996\n",
    "* 2: 1222016429.5818548\n",
    "* 3: 697675985.9232541\n",
    "* 4: 561449869.0233672\n",
    "* 5: 459598615.93620276\n",
    "* 6: 370670666.4551599\n",
    "* 7: 326371468.84605604\n",
    "* 8: 292667190.21452206\n",
    "* 9: 262847830.99261418\n",
    "* 10: 240391251.9439149\n",
    "\n",
    "Gwählt 6, weil ab da die WCSS nicht mehr so stark sinkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train auf cluster 6\n",
    "\n",
    "# # create and fit k-means model\n",
    "# kmeans = KMeans(n_clusters=8)\n",
    "# kmeans.fit(df_sliding)\n",
    "\n",
    "# # predict cluster labels for new data\n",
    "# labels = kmeans.predict(df_sliding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe out of df_sliding\n",
    "df_sliding = pd.DataFrame(df_sliding)\n",
    "df_sliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba529b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_labels = df_sliding.copy()\n",
    "df_both_labels['Label'] = y_true\n",
    "df_both_labels['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88987cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the label and cluster columns in a line chart\n",
    "df_both_labels.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_labels['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "#show the part around error 1\n",
    "\n",
    "df_both_labels.loc[(df_both_labels['timestamp'] >= err_1_start - dt.timedelta(0,600)) & (df_both_labels['timestamp'] <= err_1_end + dt.timedelta(0,16000)), 'Label'].plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98586873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_labels['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "around_err_1 = df_both_labels.loc[(df_both_labels['timestamp'] >= err_3_start- dt.timedelta(0,600)) & (df_both_labels['timestamp'] <= err_3_end + dt.timedelta(0,16000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "around_err_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b46796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the labels around err1\n",
    "\n",
    "around_err_1.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliding_with_both_lables = df_sliding.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be3fcc",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd081ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) | \n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = MetroBinaryClassifier(use_shortened=False, n_components=0.95, window_size=60, test_size=0.3, random_state=0, max_depth=3, min_samples_leaf=10, excluded_columns=['timestamp', 'Label', 'gpsLong', 'gpsLat', 'gpsSpeed', 'gpsQuality'])\n",
    "df_sliding = p.sliding_window(df, 60)\n",
    "y_true = df['Label']\n",
    "df_sliding.drop(['Label'], axis=1, inplace=True)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "df_sliding = pca.fit_transform(df_sliding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAuerT SEHR LANGE!\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize the DBSCAN model\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "\n",
    "# Fit the model to your data\n",
    "dbscan.fit(df_sliding)\n",
    "\n",
    "# Obtain the cluster labels for each data point\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37abb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620df00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2f6a2ec",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "Ansatz GaussianMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) | \n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aefa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliding = p.sliding_window(df, 60)\n",
    "y_true = df_sliding['Label']\n",
    "\n",
    "#scale data\n",
    "\n",
    "df_sliding.drop(['Label'], axis=1, inplace=True)\n",
    "df_sliding = StandardScaler().fit_transform(df_sliding) # normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d397c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create an instance of the GMM class\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "\n",
    "# Fit the model to your data\n",
    "gmm.fit(df_sliding)\n",
    "\n",
    "# Predict the cluster assignments for each data point\n",
    "cluster_labels = gmm.predict(df_sliding)\n",
    "\n",
    "# Get the cluster means\n",
    "cluster_means = gmm.means_\n",
    "\n",
    "# Get the cluster covariances\n",
    "cluster_covariances = gmm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d2193",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782443bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the labels and the cluster labels\n",
    "labels_df = pd.DataFrame({'Label': y_true, 'Cluster': cluster_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607691cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the parts where label is 1\n",
    "labels_df.loc[labels_df['Label'] == 1].plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e193cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.plot.scatter(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define threshold\n",
    "threshold = 3\n",
    "\n",
    "# Define function to apply to rolling window\n",
    "def check_consecutive(window):\n",
    "    current_value = window.iloc[0]\n",
    "    consecutive_count = sum(window == current_value)\n",
    "    if consecutive_count >= threshold:\n",
    "        return current_value\n",
    "    else:\n",
    "        return window.iloc[-1]  \n",
    "\n",
    "new = pd.DataFrame()\n",
    "# Apply function to rolling window\n",
    "new['smooth_cluster'] = labels_df['Cluster'].rolling(threshold).apply(check_consecutive, raw=False)\n",
    "\n",
    "# Forward fill to propagate the last valid value\n",
    "new['smooth_cluster'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "new['Lable'] = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31225bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a plotly plot from the same data as above\n",
    "fig = px.line(labels_df.loc[labels_df['Label'] == 1], y=['Cluster', 'Label'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115b1e5",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "#### Noch probieren:\n",
    "* K-Means mit rohdaten(kein rolling window)\n",
    "* Probieren 2 Sensorwerte plotten für kmeans\n",
    "* Probieren nur wichtige collumns für kmeans (kein PCA sondern reines ausschneiden der werte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datensatz einlsesn \n",
    "df = pd.read_csv('dataset_train.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "#Label erstellen\n",
    "df['Label'] = np.where(\n",
    "    ((df['timestamp'] >= err_1_start) & (df['timestamp'] <= err_1_end)) |\n",
    "    ((df['timestamp'] >= err_2_start) & (df['timestamp'] <= err_2_end)) |\n",
    "    ((df['timestamp'] >= err_3_start) & (df['timestamp'] <= err_3_end)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a50f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betrachtet werden hier jetzt mal nur TP3 und Oil_Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42881852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index of the row that has df['timestamp] == err_1_start\n",
    "err_1_start_index = df.index[df['timestamp'] == err_1_start].tolist()[0]\n",
    "err_1_end_index = df.index[df['timestamp'] == err_1_end].tolist()[0]\n",
    "err_2_start_index = df.index[df['timestamp'] == err_2_start].tolist()[0]\n",
    "err_2_end_index = df.index[df['timestamp'] == err_2_end].tolist()[0]\n",
    "err_3_start_index = df.index[df['timestamp'] == err_3_start].tolist()[0]\n",
    "err_3_end_index = df.index[df['timestamp'] == err_3_end].tolist()[0]\n",
    "#print both indexes\n",
    "print(\"Start Idx: \", err_1_start_index, \"| End Idx: \", err_1_end_index)\n",
    "\n",
    "#add 10 minutes to err1 start index\n",
    "err_1_start_index_t = err_1_start_index - 60000 # 10 minutes (1 datapoint per second)\n",
    "\n",
    "print(\"Start Idx: \", err_1_start_index_t, \"| End Idx: \", err_1_end_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot label between err_1_start_index_t and err_1_end_index\n",
    "df.loc[err_1_start_index_t:err_1_end_index, 'Label'].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the data between err_1_start_index_t and err_1_end_index\n",
    "df.loc[err_1_start_index_t:err_1_end_index].plot.line(x='timestamp', y=['TP3', 'Oil_temperature', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the oil temperature against TP3 between err_1_start_index_t and err_1_end_index in a scatter plot\n",
    "df.loc[err_1_start_index_t:err_1_end_index].plot.scatter(x='TP3', y='Oil_temperature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e505202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature']]\n",
    "\n",
    "#scale data\n",
    "df_tmp = pd.DataFrame(StandardScaler().fit_transform(df_tmp),columns=df_tmp.columns) # normalize data\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc0f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df774934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the weights of the clusters\n",
    "kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686eacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the oil temperature against TP3 between err_1_start_index_t and err_1_end_index in a scatter plot and include the cluster centers\n",
    "df_tmp.plot.scatter(x='TP3', y='Oil_temperature')\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='red', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c732f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = kmeans.predict(df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.plot.scatter(x='TP3', y='Oil_temperature', c='Cluster', colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame()\n",
    "df_labels['Label'] = df.loc[err_1_start_index_t:err_1_end_index, 'Label']\n",
    "df_labels['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we try it with 3 sensors\n",
    "\n",
    "df_3d = df.loc[err_1_start_index_t:err_1_end_index, ['TP3', 'Oil_temperature', 'Reservoirs']].copy()\t\n",
    "\n",
    "#scale data\n",
    "df_3d = pd.DataFrame(StandardScaler().fit_transform(df_3d),columns=df_3d.columns) # normalize data\n",
    "\n",
    "#perform kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(df_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d plot of the data\n",
    "fig = px.scatter_3d(df_3d, x='TP3', y='Oil_temperature', z='Reservoirs', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the labeling\n",
    "df_labels = pd.DataFrame()\n",
    "df_labels['Label'] = df.loc[err_1_start_index_t:err_1_end_index, 'Label']\n",
    "df_labels['Cluster'] = kmeans.labels_\n",
    "\n",
    "df_labels.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7629b3",
   "metadata": {},
   "source": [
    "--------------\n",
    "KMeans nur mit den wichtigsten collumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove label from the excluded list\n",
    "excluded.remove('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all excluded columns\n",
    "df_essentials = df.drop(columns=excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit kemans in the test piod between err_1_start_index_t and err_1_end_index\n",
    "df_essentails_test = df_essentials.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "#scale the data\n",
    "l = df_essentails_test['Label']\n",
    "df_essentails_test = df_essentails_test.drop(columns=['Label'], axis=1)\n",
    "df_essentails_test = pd.DataFrame(MinMaxScaler().fit_transform(df_essentails_test),columns=df_essentails_test.columns) # normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(df_essentails_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23068da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e677518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essentails_test['Cluster'] = kmeans.labels_\n",
    "l = l.reset_index()\n",
    "df_essentails_test['Label'] = l['Label']\n",
    "df_essentails_test.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df31866",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essentials = df.drop(columns=excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOverlap(y_true_s, cluster):\n",
    "    #this function takes 2 arrays with labels and calculates how many labels are the same.\n",
    "    #the result is a percentage of how many labels are the same\n",
    "    #the higher the percentage the better the clustering\n",
    "    #every position in one array is checked against the other array\n",
    "    #after that the labels of the clustering are switched and the process is repeated, because we do not know which label is which\n",
    "    #the higher percentage is returned\n",
    "\n",
    "    #calculate the percentage of the first try\n",
    "    y_true = y_true_s.values\n",
    "\n",
    "    print(len(y_true), len(cluster))\n",
    "    percentage = 0\n",
    "    for i in range(len(y_true)-1):\n",
    "        if y_true[i] == cluster[i]:\n",
    "            percentage += 1\n",
    "    percentage = percentage / len(y_true)\n",
    "\n",
    "    #calculate the percentage of the second try\n",
    "    percentage2 = 0\n",
    "    for i in range(len(y_true)-1):\n",
    "        if y_true[i] != cluster[i]:\n",
    "            percentage2 += 1\n",
    "    percentage2 = percentage2 / len(y_true)\n",
    "\n",
    "    #return the higher percentage\n",
    "    if percentage > percentage2:\n",
    "        return percentage\n",
    "    else:\n",
    "        return percentage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e60493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a loop where we repeat the process, but we alwas remove one colum from the dataframe, and comapre the results\n",
    "\n",
    "accuracys = []\n",
    "for column in df_essentials.columns:\n",
    "    if(column == 'Label'):\n",
    "        continue\n",
    "    print(\"Without: \", column)\n",
    "    copy = df_essentials.copy()\n",
    "    copy = copy.drop(columns=column, axis=1,inplace=False )\n",
    "    print(\"columns: \", copy.columns)\n",
    "    df_essentails_test = copy.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "    y = df_essentails_test['Label']\n",
    "    df_essentails_test.drop(columns='Label', axis=1,inplace=True )\n",
    "\n",
    "    #scale the data\n",
    "    df_essentails_test = pd.DataFrame(MinMaxScaler().fit_transform(df_essentails_test),columns=df_essentails_test.columns) # normalize data\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(df_essentails_test)\n",
    "    df_essentails_test['Cluster'] = kmeans.labels_\n",
    "    df_essentails_test['Label'] = y\n",
    "    #df_essentails_test.plot.line(y=['Cluster', 'Label'])\n",
    "    print(checkOverlap(y, kmeans.labels_))\n",
    "    accuracys.append((checkOverlap(y, kmeans.labels_), column))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print orderd accuracys\n",
    "accuracys.sort(reverse=True)\n",
    "accuracys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same thing as the loop above, but now we remove 2 columns\n",
    "accuracys2 = []\n",
    "for column in df_essentials.columns:\n",
    "    if(column == 'Label'):\n",
    "        continue\n",
    "    for column2 in df_essentials.columns:\n",
    "        if(column2 == 'Label' or column2 == column):\n",
    "            continue\n",
    "        print(\"Without: \", column, column2)\n",
    "        copy = df_essentials.copy()\n",
    "        copy = copy.drop(columns=[column, column2], axis=1,inplace=False )\n",
    "        print(\"columns: \", copy.columns)\n",
    "        df_essentails_test = copy.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "        y = df_essentails_test['Label']\n",
    "        df_essentails_test.drop(columns='Label', axis=1,inplace=True )\n",
    "        #scale the data\n",
    "        df_essentails_test = pd.DataFrame(MinMaxScaler().fit_transform(df_essentails_test),columns=df_essentails_test.columns) # normalize data\n",
    "        kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(df_essentails_test)\n",
    "        df_essentails_test['Cluster'] = kmeans.labels_\n",
    "        df_essentails_test['Label'] = y\n",
    "        #df_essentails_test.plot.line(y=['Cluster', 'Label'])\n",
    "        print(checkOverlap(y, kmeans.labels_))\n",
    "        accuracys2.append((checkOverlap(y, kmeans.labels_), column, column2))\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys2.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30180166",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try cmeans with te best 2 columns\n",
    "copy = df_essentials.copy()\n",
    "copy = copy.drop(columns=['TP2', 'Flowmeter'], axis=1,inplace=False)\n",
    "print(\"columns: \", copy.columns)\n",
    "df_essentails_test = copy.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "y = df_essentails_test['Label']\n",
    "df_essentails_test.drop(columns='Label', axis=1,inplace=True )\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(df_essentails_test)\n",
    "df_essentails_test['Cluster'] = kmeans.labels_\n",
    "df_essentails_test['Label'] = y\n",
    "df_essentails_test.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no try kmeans on the entire dataset\n",
    "\n",
    "copy = df_essentials.copy()\n",
    "\n",
    "copy = copy.drop(columns=['TP2', 'Flowmeter'], axis=1,inplace=False)\n",
    "print(\"columns: \", copy.columns)\n",
    "df_essentails_test = copy.copy()\n",
    "y = df_essentails_test['Label']\n",
    "df_essentails_test.drop(columns='Label', axis=1,inplace=True )\n",
    "#scale the data\n",
    "df_essentails_test = pd.DataFrame(MinMaxScaler().fit_transform(df_essentails_test),columns=df_essentails_test.columns) # normalize data\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(df_essentails_test)\n",
    "df_essentails_test['Cluster'] = kmeans.labels_\n",
    "df_essentails_test['Label'] = y\n",
    "df_essentails_test.plot.line(y=['Cluster', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded.remove('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform pca to reduce the dimensions to 3 dimensions, to make i tpossible to plot the data\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "#excluded.remove('Label')\n",
    "df_essentials = df.drop(columns=excluded)\n",
    "y = df_essentials['Label']\n",
    "df_essentials.drop(columns=['Label'], axis=1,inplace=True )\n",
    "#scale the data\n",
    "df_essentials = pd.DataFrame(MinMaxScaler().fit_transform(df_essentials),columns=df_essentials.columns) # normalize data\n",
    "\n",
    "principalComponents = pca.fit_transform(df_essentials)\n",
    "pc = pd.DataFrame(principalComponents, columns=['PC1', 'PC2', 'PC3'])\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21782049",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_test = pc.loc[err_1_start_index_t:err_1_end_index, :].copy()\n",
    "pc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(pc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d scatterplot of the data\n",
    "fig = px.scatter_3d(pc_test, x='PC1', y='PC2', z='PC3', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = y.loc[err_1_start_index_t:err_1_end_index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pc_test, x='PC1', y='PC2', z='PC3', color=labs)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19622f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pc_test.index, y=labs,\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "fig.add_trace(go.Scatter(x=pc_test.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef0f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1817e67",
   "metadata": {},
   "source": [
    "-------\n",
    "try with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7abd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(data: DataFrame, window_size = 60):\n",
    "        print(\"SL\")\n",
    "        columns = data.columns\n",
    "        operations = ['mean', 'std', 'min', 'max']\n",
    "\n",
    "        #generate a dict, where the keys are the column names and the values are the operations that should be performed on the column\n",
    "        operations_dict = {}\n",
    "        for column in columns:\n",
    "            if(column == 'Label'):\n",
    "                continue\n",
    "            operations_dict[column] = operations\n",
    "\n",
    "        print(operations_dict)\n",
    "        df_rolling = data.rolling(window_size).agg(operations_dict)\n",
    "\n",
    "        #drop all columns that have NaN values\n",
    "        df_rolling = df_rolling.dropna()\n",
    "\n",
    "        #flatten df_rolling\n",
    "        df_rolling.columns = ['_'.join(col) for col in df_rolling.columns]\n",
    "\n",
    "        #keep original LabelValues\n",
    "        df_rolling['Label'] = data['Label']\n",
    "\n",
    "        #rename Label_min to Label\n",
    "        #df_rolling = df_rolling.rename(columns={'Label_max': 'Label'})\n",
    "        return df_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df.drop(columns=excluded)\n",
    "\n",
    "#try with only TP3, oiltemp and \n",
    "\n",
    "df_s = df_s.loc[err_1_start_index_t:err_1_end_index].copy()\n",
    "\n",
    "df_ss= sliding_window(df_s, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b448da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "df_ss = pd.DataFrame(MinMaxScaler().fit_transform(df_ss),columns=df_ss.columns) # normalize data\n",
    "\n",
    "#perform pca to reduce the dimensions to 3 dimensions, to make i tpossible to plot the data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "y = df_ss['Label']\n",
    "df_ss.drop(columns=['Label'], axis=1,inplace=True )\n",
    "principalComponents = pca.fit_transform(df_ss)\n",
    "pc = pd.DataFrame(principalComponents, columns=['PC1', 'PC2', 'PC3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ad8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d scatter plot with actual labels\n",
    "fig = px.scatter_3d(pc, x='PC1', y='PC2', z='PC3', color=y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform kmeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(pc)\n",
    "#3d scatterplot of the data\n",
    "fig = px.scatter_3d(pc, x='PC1', y='PC2', z='PC3', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187201b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------- see what happens without pca ----------------------------\n",
    "\n",
    "#perform kmeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(df_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_ss.index, y=y,\n",
    "                    mode='lines',\n",
    "                    name='y'))\n",
    "fig.add_trace(go.Scatter(x=df_ss.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='clusters'))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- try with only oil_temp TP3 and reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68732c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "analog_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b36dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['TP3', 'Reservoirs', 'Oil_temperature', 'Label']\n",
    "\n",
    "#generate a df where only the columns in keep are present\n",
    "df_small = df[keep]\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf557de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d plot the data\n",
    "df_small = df_small.loc[err_1_start_index_t:err_1_end_index].copy()\n",
    "fig = px.scatter_3d(df_small, x='TP3', y='Reservoirs', z='Oil_temperature', color='Label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "#import scaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_small['Label']\n",
    "df_small.drop(columns=['Label'], axis=1,inplace=True )\n",
    "\n",
    "#scale the data\n",
    "df_small = pd.DataFrame(MinMaxScaler().fit_transform(df_small),columns=df_small.columns) # normalize data\n",
    "\n",
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(df_small)\n",
    "\n",
    "#scatter 3d\n",
    "fig = px.scatter_3d(df_small, x='TP3', y='Reservoirs', z='Oil_temperature', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4aba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_small.index, y=y,\n",
    "                    mode='lines',\n",
    "                    name='y'))\n",
    "fig.add_trace(go.Scatter(x=df_small.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='clusters'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try on the entire dataset\n",
    "df_small = df[keep]\n",
    "\n",
    "df_small = sliding_window(df_small, 60)\n",
    "\n",
    "#scale the data\n",
    "y = df_small['Label']\n",
    "df_small.drop(columns=['Label'], axis=1,inplace=True )\n",
    "\n",
    "\n",
    "df_small = pd.DataFrame(MinMaxScaler().fit_transform(df_small),columns=df_small.columns) # normalize data\n",
    "\n",
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(df_small)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot real and predicted labels in matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y, label='y')\n",
    "ax.plot(kmeans.labels_, label='clusters')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = pd.DataFrame({'y':y, 'clusters':kmeans.labels_})\n",
    "df_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a90a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show df_l around error 1\n",
    "df_l.loc[err_2_start_index-600:err_2_end_index+2000].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da869867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create an instance of the GMM class\n",
    "gmm = GaussianMixture(n_components=4)\n",
    "\n",
    "# Fit the model to your data\n",
    "gmm.fit(df_small)\n",
    "\n",
    "# Predict the cluster assignments for each data point\n",
    "cluster_labels = gmm.predict(df_small)\n",
    "\n",
    "# Get the cluster means\n",
    "cluster_means = gmm.means_\n",
    "\n",
    "# Get the cluster covariances\n",
    "cluster_covariances = gmm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb97420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = pd.DataFrame({'y':y, 'clusters':cluster_labels})\n",
    "df_l.loc[err_1_start_index_t:err_1_end_index].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e312b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d scatter plot around error 3\n",
    "fig = px.scatter_3d(df_small.loc[err_3_start_index-600:err_3_end_index+2000], x='TP3', y='Reservoirs', z='Oil_temperature', color=cluster_labels[err_3_start_index-600:err_3_end_index+2000+1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa823a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(y, label='y')\n",
    "ax.plot(cluster_labels, label='clusters')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dbb1a",
   "metadata": {},
   "source": [
    "* Reservoirs_mean        0.123420\n",
    "* Reservoirs_min         0.062413\n",
    "* DV_pressure_mean       0.060066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842db2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drölf = df.copy()\n",
    "test_drölf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e188a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drölf = df.copy()\n",
    "#excluded.remove('Label')\n",
    "#remove exluded columns\n",
    "test_drölf.drop(columns=excluded, axis=1,inplace=True )\n",
    "\n",
    "test_drölf = sliding_window(test_drölf, 60)\n",
    "\n",
    "keep = ['Reservoirs_mean', 'Reservoirs_min', 'DV_pressure_mean', 'Label']\n",
    "\n",
    "test_drölf = test_drölf[keep]\n",
    "\n",
    "test_drölf = pd.DataFrame(MinMaxScaler().fit_transform(test_drölf),columns=test_drölf.columns) # normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d scatter plot around error 1\n",
    "s = test_drölf.loc[err_1_start_index-600:err_1_end_index+2000]\n",
    "fig = px.scatter_3d(s, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=s.index, size=s['Label']+1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test_drölf['Label']\n",
    "y_s = s['Label']\n",
    "s.drop(columns=['Label'], axis=1,inplace=True)\n",
    "test_drölf.drop(columns=['Label'], axis=1,inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0,n_init=10).fit(s)\n",
    "\n",
    "#scatter 3d\n",
    "s = test_drölf.loc[err_1_start_index-600:err_1_end_index+2000]\n",
    "fig = px.scatter_3d(s, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=kmeans.labels_)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4584f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "y_cut = y[err_1_start_index]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=s.index, y=y_s,\n",
    "                    mode='lines',\n",
    "                    name='y'))\n",
    "fig.add_trace(go.Scatter(x=s.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='clusters'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do it on the entire dataset\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(test_drölf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'y':y, 'clusters':kmeans.labels_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d324e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = res.loc[err_1_start_index-600:err_1_end_index+2000]\n",
    "res_2 = res.loc[err_2_start_index-600:err_2_end_index+2000]\n",
    "res_3 = res.loc[err_3_start_index-600:err_3_end_index+2000]\n",
    "\n",
    "slice1 = test_drölf.loc[err_1_start_index-600:err_1_end_index+2000]\n",
    "slice2 = test_drölf.loc[err_2_start_index-600:err_2_end_index+2000]\n",
    "slice3 = test_drölf.loc[err_3_start_index-600:err_3_end_index+2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_3.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ba17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #3d scatter\n",
    "# fig = px.scatter_3d(slice1, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=res_1['clusters'])\n",
    "# fig.show()\n",
    "# fig = px.scatter_3d(slice1, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=res_1['y'])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7942032",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.concat([slice1, slice2, slice3])\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 3 slices together, \n",
    "cl = pd.concat([res_1, res_2, res_3])\n",
    "\n",
    "#scatter plot\n",
    "fig = px.scatter_3d(cat, x='Reservoirs_mean', y='Reservoirs_min', z='DV_pressure_mean', color=cl['clusters'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869a51c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ca2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c074e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d71c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb91db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d451ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded.remove('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eab252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll = df.drop(columns=excluded, axis=1,inplace=False)\n",
    "y_ll = df_ll['Label']\n",
    "\n",
    "df_ll.drop(columns=['Label'], axis=1,inplace=True)\n",
    "\n",
    "df_ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale\n",
    "df_ll_scale = pd.DataFrame(MinMaxScaler().fit_transform(df_ll),columns=df_ll.columns) # normalize data\n",
    "df_ll_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll_short = df_ll_scale.loc[err_1_start_index-20000:err_1_end_index-15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the oil temperature\n",
    "df_ll_short['Oil_temperature'].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b01a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import kmeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = FeatureGeneration()\n",
    "\n",
    "df_features = ft.getFeatures(df, 2048, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff04cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_scale = pd.DataFrame(MinMaxScaler().fit_transform(df_features),columns=df_features.columns) # normalize data\n",
    "\n",
    "y = df_features_scale['Error']\n",
    "df_features_scale.drop(columns=['Error'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ae494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0,n_init=10).fit(df_features_scale)\n",
    "\n",
    "#plot real and predicted labels\n",
    "clusters = kmeans.labels_\n",
    "#make a line graph\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_features_scale.index, y=y,\n",
    "                    mode='lines',\n",
    "                    name='y'))\n",
    "fig.add_trace(go.Scatter(x=df_features_scale.index, y=clusters,\n",
    "                    mode='lines',\n",
    "                    name='clusters'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827983e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('MachineLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc9b5ad24f7c64b5a327ba84ead6ad3fbc2ebc7246c1e6195a5d7b169b73b46a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
